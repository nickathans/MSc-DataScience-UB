{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vQ-jBm1wNcKS",
        "wMSkmW64sPP1",
        "ZfCxd9PRbZeZ",
        "ng-l2nJ9de8D",
        "1lzl68C11eix"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recommender Systems: Lab. 3\n",
        "\n",
        "#### Nikolaos Athanasopoulos\n",
        "#### Zoltan Kunos"
      ],
      "metadata": {
        "id": "LyzfIO8agDcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle Collaborative Filtering Approach"
      ],
      "metadata": {
        "id": "vQ-jBm1wNcKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "from google.colab import files\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kY-tUkG_hiCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns={'release_date': 'type','sex': 'age', 'age': 'sex'}, inplace=True)"
      ],
      "metadata": {
        "id": "pZV_GlSWW_ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "s_5SJI2hXPy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create a function that allows us to divide the dataset into:\n",
        "#### training and test\n",
        "def assign_to_set(df):\n",
        "    sampled_ids = np.random.choice(df.index,\n",
        "                                   size=np.int64(np.ceil(df.index.size * 0.2)),\n",
        "                                   replace=False)\n",
        "    df.loc[sampled_ids, 'for_testing'] = True\n",
        "    return df\n",
        "\n",
        "def create_train_test(data,key = 'user_id'):\n",
        "    data['for_testing'] = False\n",
        "    grouped = data.groupby(key, group_keys=False).apply(assign_to_set)\n",
        "    # dataframe used to train our model\n",
        "    data_train = data[grouped.for_testing == False]\n",
        "    # dataframe used to evaluate our model\n",
        "    data_test = data[grouped.for_testing == True]\n",
        "    return data_train, data_test\n"
      ],
      "metadata": {
        "id": "Cqn_YzDrxZpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test =  create_train_test(data)\n",
        "print(train.shape, test.shape)\n",
        "\n",
        "print(\"Training data_set has \"+ str(train.shape[0]) +\" ratings\")\n",
        "print(\"Test data set has \"+ str(test.shape[0]) +\" ratings\")\n",
        "print(\"La BD has \", data.movie_id.nunique(), \" movies\")"
      ],
      "metadata": {
        "id": "GiOtentrjnuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "pMAHyW9D03hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "qC9O3rMXwCrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rmse(y_pred, y_true):\n",
        "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
        "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
        "\n",
        "## Add another Loss Function\n",
        "def log_loss(self, pred, real):\n",
        "    \"\"\" Log loss error \"\"\"\n",
        "    return np.log(np.exp(-pred * real) + 1.0)\n",
        "\n",
        "def precision(recommended_items, relevant_items):\n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
        "    \n",
        "    return precision_score\n",
        "\n",
        "def recall(recommended_items, relevant_items):  \n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
        "    \n",
        "    return recall_score\n",
        "\n",
        "def AP(recommended_items, relevant_items):\n",
        "   \n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
        "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
        "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
        "\n",
        "    return ap_score"
      ],
      "metadata": {
        "id": "Vq6Louofj6dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(estimate_f,data_train,data_test):\n",
        "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
        "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
        "    estimated = np.array([estimate_f(u,i) if u in data_train.user_id else 3 for (u,i) in ids_to_estimate ])\n",
        "    real = data_test.rating.values\n",
        "    \n",
        "    return compute_rmse(estimated, real)\n",
        "\n",
        "\n",
        "def evaluate_algorithm_top(test, recommender_object, at=25, thr_relevant = 4):\n",
        "    \n",
        "    cumulative_precision = 0.0\n",
        "    cumulative_recall = 0.0\n",
        "    cumulative_AP = 0.0\n",
        "    \n",
        "    num_eval = 0\n",
        "\n",
        "\n",
        "    for user_id in tqdm(test.user_id.unique()):\n",
        "\n",
        "        relevant_items = test[(test.user_id==user_id )&( test.rating>=thr_relevant)].movie_id.values\n",
        "        \n",
        "        if len(relevant_items)>0:\n",
        "            \n",
        "            recommended_items = recommender_object.predict_top(user_id, at=at)\n",
        "            num_eval+=1\n",
        "\n",
        "            cumulative_precision += precision(recommended_items, relevant_items)\n",
        "            cumulative_recall += recall(recommended_items, relevant_items)\n",
        "            cumulative_AP += AP(recommended_items, relevant_items)\n",
        "            \n",
        "    cumulative_precision /= num_eval\n",
        "    cumulative_recall /= num_eval\n",
        "    MAP = cumulative_AP / num_eval\n",
        "    \n",
        "    print(\"Recommender results are: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
        "        cumulative_precision, cumulative_recall, MAP))"
      ],
      "metadata": {
        "id": "6JFcsMpNj7ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # conda install -y tqdm\n",
        "\n",
        "class CollaborativeFiltering:\n",
        "    \"\"\" Collaborative filtering using a custom sim(u,u'). \"\"\"\n",
        "    \n",
        "    def __init__(self, _type = 'user'):\n",
        "        \"\"\" Constructor \"\"\"\n",
        "        self._type = _type\n",
        "    \n",
        "    def fit(self,df_train,shrink = 10):\n",
        "        \"\"\" Prepare data structures for estimation. Similarity matrix for users \"\"\"\n",
        "        print(\"Learning...\")\n",
        "        self.df_train=df_train\n",
        "        self.urm_train = pd.pivot_table(df_train[['user_id','movie_id','rating']],columns='movie_id',index='user_id',values='rating',fill_value=0).values\n",
        "        self.index2item_id = np.array(pd.pivot_table(df_train[['user_id','movie_id','rating']],columns='user_id',index='movie_id',values='rating',fill_value=0).index)\n",
        "        self.index2user_id = np.array(pd.pivot_table(df_train[['user_id','movie_id','rating']],columns='movie_id',index='user_id',values='rating',fill_value=0).index)\n",
        "        \n",
        "        self.item_id2index = {}\n",
        "        for i in range(len(self.index2item_id)):\n",
        "            self.item_id2index[self.index2item_id[i]] = i\n",
        "            \n",
        "        self.user_id2index = {}\n",
        "        for i in range(len(self.index2user_id)):\n",
        "            self.user_id2index[self.index2user_id[i]] = i\n",
        "            \n",
        "            \n",
        "        self.num_items = len(self.index2item_id)\n",
        "        self.num_users = len(self.index2user_id)\n",
        "        self.movie_id2title = {}\n",
        "        for row in df_train[['movie_id','title']].drop_duplicates().values:\n",
        "            self.movie_id2title[row[0]] = row[1]\n",
        "        \n",
        "        \n",
        "        if(self._type =='user'): # USER BASED\n",
        "            print(\"Computing user similarities\")\n",
        "            self.sim_matrix = np.zeros((self.num_users,self.num_users))   \n",
        "            \n",
        "            user_norms = np.sqrt(np.power(self.urm_train,2).sum(axis=1)).ravel()\n",
        "            for user_index in tqdm(range(self.num_users)):\n",
        "                # compute cosine distance\n",
        "                numerator_vector = self.urm_train[user_index].dot(self.urm_train.T).ravel()\n",
        "                denominator_vector = user_norms[user_index] * user_norms + shrink + 1e-6\n",
        "\n",
        "                similarity_vector = numerator_vector/denominator_vector\n",
        "                self.sim_matrix[user_index,:] = similarity_vector\n",
        "            \n",
        "            \n",
        "        elif(self._type=='item'): ## ITEM BASED\n",
        "            print(\"Computing item similarities\")\n",
        "            self.sim_matrix = np.zeros((self.num_items,self.num_items))\n",
        "            item_norms = np.sqrt(np.power(self.urm_train,2).sum(axis=0)).ravel()\n",
        "            for item_index in tqdm(range(self.num_items)):\n",
        "                # compute cosine distance\n",
        "                numerator_vector = self.urm_train.T[item_index].dot(self.urm_train).ravel()\n",
        "                denominator_vector = item_norms[item_index] * item_norms + shrink + 1e-6\n",
        "\n",
        "                similarity_vector = numerator_vector/denominator_vector\n",
        "                self.sim_matrix[item_index,:] = similarity_vector\n",
        "                \n",
        "    def predict_score(self, user_id, movie_id):\n",
        "        if movie_id not in self.item_id2index:\n",
        "            return self.df_train.rating.mean()\n",
        "            \n",
        "        user_index = self.user_id2index[user_id]\n",
        "        item_index = self.item_id2index[movie_id]\n",
        "        \n",
        "        if(self._type=='user'):\n",
        "            rating_num = self.urm_train.T[item_index,:].dot(self.sim_matrix[user_index,:])\n",
        "            rating_den = np.sum((self.urm_train.T[item_index,:]>0).dot(self.sim_matrix[user_index,:]))\n",
        "        elif(self._type=='item'):\n",
        "            rating_num = self.urm_train[user_index,:].dot(self.sim_matrix[item_index,:])\n",
        "            rating_den = np.sum((self.urm_train[user_index,:]>0).dot(self.sim_matrix[item_index,:]))\n",
        "            \n",
        "        if rating_den == 0:\n",
        "            return self.df_train.rating.mean()\n",
        "        else:\n",
        "            return rating_num/rating_den\n",
        "        \n",
        "    def predict_top(self, user_id, at=5, remove_seen=True):\n",
        "        '''Given a user_id predict its top AT items'''\n",
        "        seen_items = self.df_train[self.df_train.user_id==user_id].movie_id.values\n",
        "        unseen_items = set(self.df_train.movie_id.values) - set(seen_items)\n",
        "\n",
        "        predictions = [(item_id,self.predict_score(user_id,item_id)) for item_id in unseen_items]\n",
        "\n",
        "        sorted_predictions = sorted(predictions, key=lambda x: x[1],reverse = True)[:at]\n",
        "        return [i[0] for i in sorted_predictions]"
      ],
      "metadata": {
        "id": "h156nQiVkAqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec_object = CollaborativeFiltering()\n",
        "rec_object.fit(train)\n",
        "rec_object.predict_score(user_id=2,movie_id=1)"
      ],
      "metadata": {
        "id": "Z4gq579QkTuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('kaggle_baseline.csv')"
      ],
      "metadata": {
        "id": "XUra_HaR0gRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# open the file in the write mode\n",
        "with open('solution.csv', 'w',encoding='UTF8') as f:\n",
        "    # create the csv writer\n",
        "    writer = csv.writer(f)\n",
        "    # write a row to the csv file\n",
        "    writer.writerow(['user_id', 'prediction'])\n",
        "    for user_id in tqdm(test.user_id.unique()):\n",
        "      try:\n",
        "          relevant_items = rec_object.predict_top(user_id, at=25)\n",
        "          list_relevants = ' '.join([str(elem) for elem in relevant_items])\n",
        "          writer.writerow([str(user_id),list_relevants])\n",
        "      except KeyError as e:\n",
        "          print(f\"Error: {e}. User ID {user_id} not found.\")\n"
      ],
      "metadata": {
        "id": "VHA69H-MkilZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first 3 approaches we followed the corresponding notebooks that are uploaded at Campus Virtual, which are the following:\n",
        "- Collaborative Filtering using SVD Decomposition\n",
        "- The Vanilla Matrix Factorization Model\n",
        "- The Vanilla Matrix Factorization Model with biases"
      ],
      "metadata": {
        "id": "-lzvRG5gMwim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative Filtering using SVD Decomposition"
      ],
      "metadata": {
        "id": "wMSkmW64sPP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "class RecSys_mf():\n",
        "    \"\"\" Collaborative filtering using SVD. \"\"\"\n",
        "    \n",
        "    def __init__(self, num_components=10):\n",
        "        \"\"\" Constructor \"\"\"\n",
        "        self.num_components=num_components\n",
        "        \n",
        "        \n",
        "        \n",
        "    def fit(self,df_train):\n",
        "        \"\"\" We decompose the R matrix into to submatrices using the training data \"\"\"\n",
        "        \n",
        "        self.train = df_train\n",
        "        self.urm = pd.pivot_table(df_train[['user_id','movie_id','rating']],columns='movie_id',index='user_id',values='rating')\n",
        "        \n",
        "        # We create a dictionary where we will store the user_id and movie_id which correspond \n",
        "        # to each index in the Rating matrix\n",
        "        \n",
        "        user_index = np.arange(len(self.urm.index))\n",
        "        self.users = dict(zip(user_index,self.urm.index ))\n",
        "        self.users_id2index = dict(zip(self.urm.index,user_index)) \n",
        "        \n",
        "        movie_index = np.arange(len(self.urm.columns))\n",
        "        self.movies = dict(zip(movie_index,self.urm.columns )) \n",
        "        self.movies_id2index= dict(zip(self.urm.columns, movie_index))\n",
        "        self.movies_index2id= dict(zip(movie_index,self.urm.columns))\n",
        "        self.movie_id2title = dict(df_train.groupby(by=['movie_id','title']).count().index)\n",
        "        \n",
        "        self.pop_items = reco.train.groupby('movie_id').count()[['rating']]\n",
        "\n",
        "        train_matrix = np.array(self.urm)\n",
        "        # we mask those nan value to fill with the mean \n",
        "        mask = np.isnan(train_matrix)\n",
        "        masked_arr = np.ma.masked_array(train_matrix, mask)\n",
        "        item_means = np.mean(masked_arr, axis=0)\n",
        "\n",
        "        # nan entries will replaced by the average rating for each item\n",
        "        train_matrix = masked_arr.filled(item_means)\n",
        "        x = np.tile(item_means, (train_matrix.shape[0],1))         \n",
        "\n",
        "        # we remove the per item average from all entries.\n",
        "        # the above mentioned nan entries will be essentially zero now\n",
        "        train_matrix = train_matrix - x\n",
        "        U, s, V = np.linalg.svd(train_matrix, full_matrices=False)\n",
        "\n",
        "        # reconstruct rating matix\n",
        "        S = np.diag(s[0:self.num_components])\n",
        "        U = U[:,0:self.num_components]\n",
        "        V = V[0:self.num_components,:]\n",
        "        S_root = sqrtm(S)\n",
        "\n",
        "        USk=np.dot(U,S_root)\n",
        "        SkV=np.dot(S_root,V)\n",
        "        Y_hat = np.dot(USk, SkV)\n",
        "        self.Y_hat = Y_hat + x\n",
        "        \n",
        "    def predict_score(self, user_id, movie_id):\n",
        "        \n",
        "        if movie_id in self.movies_id2index:\n",
        "            return self.Y_hat[self.users_id2index[user_id],self.movies_id2index[movie_id]]\n",
        "        else: # in case it is a new movie \n",
        "            return 3\n",
        "\n",
        "        \n",
        "    def predict_top(self, user_id, at=5, filter_pop = 100, remove_seen=True):\n",
        "        '''Given a user_id predict its top AT items'''\n",
        "        seen_items = self.train[self.train.user_id==user_id].movie_id.values\n",
        "        unseen_items = set(self.train.movie_id.values) - set(seen_items)\n",
        "        # filter the non popular items\n",
        "        #unseen_items = [item for item  in set(self.pop_items[self.pop_items.rating>filter_pop].index) if item in unseen_items]\n",
        "        predictions = [(item_id,self.predict_score(user_id,item_id)) for item_id in unseen_items]\n",
        "\n",
        "        sorted_predictions = sorted(predictions, key=lambda x: x[1],reverse = True)[:at]\n",
        "        return [i[0] for i in sorted_predictions]"
      ],
      "metadata": {
        "id": "ix6qCoyIsO0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(predict_f,data_train,data_test):\n",
        "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
        "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
        "    list_users = set(data_train.user_id)\n",
        "    estimated = np.array([predict_f(u,i) if u in list_users else 3 for (u,i) in ids_to_estimate ])\n",
        "    real = data_test.rating.values\n",
        "    return compute_rmse(estimated, real)\n",
        "\n",
        "def compute_rmse(y_pred, y_true):\n",
        "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
        "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
        "\n",
        "def precision(recommended_items, relevant_items):\n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
        "    \n",
        "    return precision_score\n",
        "\n",
        "def recall(recommended_items, relevant_items):  \n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
        "    \n",
        "    return recall_score\n",
        "\n",
        "def f1_score(precision, recall):\n",
        "    \"\"\" Compute F1 score. \"\"\"\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def AP(recommended_items, relevant_items):\n",
        "   \n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
        "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
        "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
        "\n",
        "    return ap_score\n",
        "\n",
        "## Divide the data in two sets: training and test\n",
        "def assign_to_set(df):\n",
        "    sampled_ids = np.random.choice(df.index,\n",
        "                                   size=np.int64(np.ceil(df.index.size * 0.2)),\n",
        "                                   replace=False)\n",
        "    df.loc[sampled_ids, 'for_testing'] = True\n",
        "    return df\n",
        "\n",
        "def evaluate_algorithm_top(test, recommender_object, at=25, thr_relevant = 0.85):\n",
        "    \n",
        "    cumulative_precision = 0.0\n",
        "    cumulative_recall = 0.0\n",
        "    cumulative_AP = 0.0\n",
        "    cumulative_f1 = 0.0\n",
        "    num_eval = 0\n",
        "\n",
        "\n",
        "    for user_id in tqdm(test.user_id.unique()):\n",
        "        \n",
        "        relevant_items = test[test.user_id==user_id]\n",
        "        thr = np.quantile(relevant_items.rating,thr_relevant)\n",
        "        relevant_items = np.array(relevant_items[relevant_items.rating >=thr].movie_id.values)\n",
        "        if len(relevant_items)>0:\n",
        "            \n",
        "            recommended_items = recommender_object.predict_top(user_id, at=at)\n",
        "            num_eval+=1\n",
        "\n",
        "            cumulative_precision += precision(recommended_items, relevant_items)\n",
        "            cumulative_recall += recall(recommended_items, relevant_items)\n",
        "            cumulative_AP += AP(recommended_items, relevant_items)\n",
        "            \n",
        "    cumulative_precision /= num_eval\n",
        "    cumulative_recall /= num_eval\n",
        "    MAP = cumulative_AP / num_eval\n",
        "    f1 = f1_score(cumulative_precision, cumulative_recall) \n",
        "    \n",
        "    print(\"Recommender results are: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}, F1 = {:.4f}\".format(\n",
        "    cumulative_precision, cumulative_recall, MAP, f1))\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "ATICR_tkzNwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "reco = RecSys_mf(num_components=30)\n",
        "reco.fit(train)\n",
        "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.predict_score,train,test))\n",
        "evaluate_algorithm_top(test, reco, at = 25)"
      ],
      "metadata": {
        "id": "Age0AGe9uwc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Vanilla Matrix Factorization Model"
      ],
      "metadata": {
        "id": "ZfCxd9PRbZeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "class RecSys_vanilla_mf(RecSys_mf):\n",
        "    \"\"\" Collaborative filtering using a custom sim(u,u'). \"\"\"\n",
        "\n",
        "    def __sdg__(self):\n",
        "        for idx in self.training_indices:\n",
        "            u = self.sample_row[idx]\n",
        "            i = self.sample_col[idx]\n",
        "            user_id = self.users[u]\n",
        "            item_id = self.movies[i]\n",
        "            \n",
        "            prediction = self.predict_score(user_id, item_id)\n",
        "            error = (self.ratings[u,i] - prediction) # error\n",
        "            \n",
        "            #Update latent factors\n",
        "            self.user_vecs[u, :] += self.learning_rate * \\\n",
        "                                    (error * self.item_vecs[i, :] - \\\n",
        "                                     self.lmbda * self.user_vecs[u,:])\n",
        "            self.item_vecs[i, :] += self.learning_rate * \\\n",
        "                                    (error * self.user_vecs[u, :] - \\\n",
        "                                     self.lmbda * self.item_vecs[i,:])\n",
        "                \n",
        "                \n",
        "    def fit(self,df_train,df_val, n_epochs = 10,learning_rate =0.001,lmbda=0.1,verbose =True):\n",
        "        \"\"\" We decompose the R matrix into to submatrices using the training data \"\"\"\n",
        "        self.train = df_train\n",
        "        self.val = df_val\n",
        "        self.urm = pd.pivot_table(df_train[['user_id','movie_id','rating']],columns='movie_id',index='user_id',values='rating')\n",
        "        \n",
        "        # We create a dictionary where we will store the user_id and movie_id which correspond \n",
        "        # to each index in the Rating matrix\n",
        "        \n",
        "        user_index = np.arange(len(self.urm.index))\n",
        "        self.users = dict(zip(user_index,self.urm.index ))\n",
        "        self.users_id2index = dict(zip(self.urm.index,user_index)) \n",
        "        \n",
        "        movie_index = np.arange(len(self.urm.columns))\n",
        "        self.movies = dict(zip(movie_index,self.urm.columns )) \n",
        "        self.movies_id2index= dict(zip(self.urm.columns, movie_index))\n",
        "        self.movies_index2id= dict(zip(movie_index,self.urm.columns))\n",
        "        self.movie_id2title = dict(df_train.groupby(by=['movie_id','title']).count().index)\n",
        "        \n",
        "        \n",
        "        self.verbose = verbose\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lmbda = lmbda\n",
        "        \n",
        "        self.mean_rating = self.train.rating.mean()\n",
        "        \n",
        "        self.ratings = np.float32(self.urm.fillna(0).values)\n",
        "        self.n_users, self.n_items = self.urm.shape\n",
        "        self.sample_row, self.sample_col = self.ratings.nonzero()\n",
        "        self.n_samples = len(self.sample_row)\n",
        "        \n",
        "        self.train_rmse =[]\n",
        "        self.test_rmse = []\n",
        "        iter_diff = 0\n",
        "        \n",
        "        # initialize latent vectors\n",
        "        self.user_vecs = self.mean_rating*np.random.normal(scale=1./self.num_components,\\\n",
        "                                          size=(self.n_users, self.num_components))\n",
        "        self.item_vecs = self.mean_rating*np.random.normal(scale=1./self.num_components,\n",
        "                                          size=(self.n_items, self.num_components))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            \n",
        "            self.training_indices = np.arange(self.n_samples)\n",
        "            \n",
        "            #shuffle training samples\n",
        "            np.random.shuffle(self.training_indices)\n",
        "            self.__sdg__()\n",
        "            \n",
        "            self.train_rmse.append(evaluate(self.predict_score,self.train,self.train))\n",
        "            self.test_rmse.append(evaluate(self.predict_score,self.train,self.val))\n",
        "            \n",
        "            \n",
        "            print('\\tTrain rmse: %s' % self.train_rmse[-1])\n",
        "            print('\\tTest rmse: %s' % self.test_rmse[-1])\n",
        "            \n",
        "        \n",
        "        if(self.verbose):\n",
        "            self.__plot_learning_curves__()\n",
        "    \n",
        "    def __plot_learning_curves__(self):\n",
        "        plt.plot(self.train_rmse,'--o',label=\"train_error\")\n",
        "        plt.plot(self.test_rmse,'--o',label=\"test_error\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        \n",
        "    def predict_score(self, user_id, movie_id):\n",
        "        \"\"\" Single user and item prediction.\"\"\"\n",
        "        user_index = self.users_id2index[user_id]\n",
        "        if movie_id in self.movies_id2index:\n",
        "            item_index = self.movies_id2index[movie_id]\n",
        "            prediction =  self.user_vecs[user_index, :].dot(self.item_vecs[item_index, :].T)\n",
        "        else:\n",
        "            prediction = self.mean_rating # this is a new movie\n",
        "        \n",
        "        return prediction\n",
        "    "
      ],
      "metadata": {
        "id": "kSVVoerobbQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = RecSys_vanilla_mf(num_components=5)\n",
        "reco.fit(train,test, n_epochs = 5,learning_rate=0.01,lmbda=0.5)\n",
        "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.predict_score,train,test))"
      ],
      "metadata": {
        "id": "yF1O4coIbklV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = RecSys_vanilla_mf(num_components=5)\n",
        "reco.fit(train,test, n_epochs = 5,learning_rate=0.01)\n",
        "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.predict_score,train,test))"
      ],
      "metadata": {
        "id": "jLUPFkR_b-PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = RecSys_vanilla_mf(num_components=5)\n",
        "reco.fit(train,test, n_epochs = 15,learning_rate=0.02)\n",
        "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.predict_score,train,test))"
      ],
      "metadata": {
        "id": "3i7KWVibcI21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "movie_id = 1 #'Toy Story (1995)'\n",
        "movie_id = 364 #'Lion King, The (1994)'\n",
        "\n",
        "pairwise_distances = euclidean_distances(reco.item_vecs, reco.item_vecs)\n",
        "[(reco.movie_id2title[reco.movies_index2id[item]],\n",
        "  item) for item in np.argsort(pairwise_distances[reco.movies_id2index[movie_id]])[0:6]]"
      ],
      "metadata": {
        "id": "nC6q1HyqcR_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_algorithm_top(test, reco, at = 25)"
      ],
      "metadata": {
        "id": "SdtpmbkacZAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Vanilla Matrix Factorization Model with biases"
      ],
      "metadata": {
        "id": "ng-l2nJ9de8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy import sparse\n",
        "\n",
        "class RecSys_vanilla_mf_biases(RecSys_vanilla_mf):\n",
        "                  \n",
        "    def __sdg__(self):\n",
        "        for idx in self.training_indices:\n",
        "            u = self.sample_row[idx]\n",
        "            i = self.sample_col[idx]\n",
        "            user_id = self.users[u]\n",
        "            item_id = self.movies[i]\n",
        "            \n",
        "            prediction = self.predict_score(user_id, item_id)\n",
        "            error = (self.ratings[u,i] - prediction) # error\n",
        "            #Update latent factors\n",
        "            self.user_vecs[u, :] += self.learning_rate * \\\n",
        "                                    (error * self.item_vecs[i, :] - self.lmbda * self.user_vecs[u,:])\n",
        "            self.item_vecs[i, :] += self.learning_rate * \\\n",
        "                                    (error * self.user_vecs[u, :] - self.lmbda * self.item_vecs[i,:])\n",
        "            \n",
        "            self.bias_item[i] += self.learning_rate * (error - self.lmbda * self.bias_item[i]) \n",
        "            self.bias_user[u] += self.learning_rate * (error - self.lmbda * self.bias_user[u]) \n",
        "            \n",
        "                \n",
        "    def fit(self,df_train,df_val, n_epochs = 10,learning_rate =0.001,lmbda=0.1,verbose =True):\n",
        "        \"\"\" Train the model. \"\"\"\n",
        "        self.verbose = verbose\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lmbda = lmbda\n",
        "        \n",
        "        self.train = df_train\n",
        "        self.val = df_val\n",
        "        \n",
        "        self.urm = pd.pivot_table(df_train[['user_id','movie_id','rating']],columns='movie_id',index='user_id',values='rating')\n",
        "        \n",
        "        # We create a dictionary where we will store the user_id and movie_id which correspond \n",
        "        # to each index in the Rating matrix\n",
        "        \n",
        "        user_index = np.arange(len(self.urm.index))\n",
        "        self.users = dict(zip(user_index,self.urm.index ))\n",
        "        self.users_id2index = dict(zip(self.urm.index,user_index)) \n",
        "        \n",
        "        movie_index = np.arange(len(self.urm.columns))\n",
        "        self.movies = dict(zip(movie_index,self.urm.columns )) \n",
        "        self.movies_id2index= dict(zip(self.urm.columns, movie_index))\n",
        "        self.movies_index2id= dict(zip(movie_index,self.urm.columns))\n",
        "        self.movie_id2title = dict(df_train.groupby(by=['movie_id','title']).count().index)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.mean_rating = self.train.rating.mean()\n",
        "        \n",
        "        self.ratings = np.float32(self.urm.fillna(0).values)\n",
        "        self.n_users, self.n_items = self.urm.shape\n",
        "        self.sample_row, self.sample_col = self.ratings.nonzero()\n",
        "        self.n_samples = len(self.sample_row)\n",
        "        print(self.n_samples)\n",
        "        self.train_rmse =[]\n",
        "        self.test_rmse = []\n",
        "        iter_diff = 0\n",
        "        \n",
        "        # initialize latent vectors\n",
        "        self.user_vecs = self.mean_rating*np.random.normal(scale=1./self.num_components,\\\n",
        "                                          size=(self.n_users, self.num_components))\n",
        "        self.item_vecs = self.mean_rating*np.random.normal(scale=1./self.num_components,\n",
        "                                          size=(self.n_items, self.num_components))\n",
        "        self.bias_item = np.random.normal(scale=1/self.n_items,size=(self.n_items))\n",
        "        self.bias_user = np.random.normal(scale=1/self.n_users,size=(self.n_users))\n",
        "        \n",
        "        \n",
        "        for epoch in range(n_epochs):\n",
        "    \n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            \n",
        "            self.training_indices = np.arange(self.n_samples)\n",
        "            \n",
        "            #shuffle training samples\n",
        "            np.random.shuffle(self.training_indices)\n",
        "            self.__sdg__()\n",
        "            \n",
        "            self.train_rmse.append(evaluate(reco.predict_score,self.train,self.train))\n",
        "            self.test_rmse.append(evaluate(reco.predict_score,self.train,self.val))\n",
        "            \n",
        "            print('\\tTrain rmse: %s' % self.train_rmse[-1])\n",
        "            print('\\tTest rmse: %s' % self.test_rmse[-1])\n",
        "        \n",
        "        if(self.verbose):\n",
        "            self.__plot_learning_curves__()\n",
        "    \n",
        "    def predict_score(self, user_id, movie_id):\n",
        "        \"\"\" Single user and item prediction.\"\"\"\n",
        "        user_index = self.users_id2index[user_id]\n",
        "        if movie_id in self.movies_id2index:\n",
        "            item_index = self.movies_id2index[movie_id]\n",
        "            prediction =  self.mean_rating + self.user_vecs[user_index, :].dot(self.item_vecs[item_index, :].T) + self.bias_item[item_index] + self.bias_user[user_index]\n",
        "        else:\n",
        "            prediction = self.mean_rating # this is a new movie\n",
        "\n",
        "        return prediction\n",
        "    "
      ],
      "metadata": {
        "id": "pJafZTwadhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = RecSys_vanilla_mf_biases(num_components=5)\n",
        "reco.fit(train,test, n_epochs = 5,learning_rate=0.02)\n",
        "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.predict_score,train,test))"
      ],
      "metadata": {
        "id": "zuM7VO1edjvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_algorithm_top(test, reco, at = 25)"
      ],
      "metadata": {
        "id": "2qJIGctWdxBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "movie_id = 1 #'Toy Story (1995)'\n",
        "#movie_id = 364 #'Lion King, The (1994)'\n",
        "\n",
        "pairwise_distances = euclidean_distances(reco.item_vecs, reco.item_vecs)\n",
        "[(reco.movie_id2title[reco.movies_index2id[item]],\n",
        "  item) for item in np.argsort(pairwise_distances[reco.movies_id2index[movie_id]])[0:6]]"
      ],
      "metadata": {
        "id": "TFDzS8BehHNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = RecSys_vanilla_mf_biases(num_components=100)\n",
        "reco.fit(train,test, n_epochs = 50,learning_rate=0.02,lmbda=0.1)"
      ],
      "metadata": {
        "id": "X8nMaY9khKwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "movie_id = 1 #'Toy Story (1995)'\n",
        "#movie_id = 364 #'Lion King, The (1994)'\n",
        "\n",
        "pairwise_distances = euclidean_distances(reco.item_vecs, reco.item_vecs)\n",
        "[(reco.movie_id2title[reco.movies_index2id[item]],\n",
        "  item) for item in np.argsort(pairwise_distances[reco.movies_id2index[movie_id]])[0:6]]"
      ],
      "metadata": {
        "id": "v4q9rCfFhUjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_algorithm_top(test, reco, at = 25)"
      ],
      "metadata": {
        "id": "fS4kDAn7hX7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sampling the input data\n",
        "- Changing the Loss Function"
      ],
      "metadata": {
        "id": "Aiqd6eMmveQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Approach that we finally have selected and worked with"
      ],
      "metadata": {
        "id": "Y6dX59lMNKdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Factorization with Keras"
      ],
      "metadata": {
        "id": "1lzl68C11eix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix Factorization (MF) is a popular approach for building movie recommender systems. The main idea behind MF is to represent the user-item interaction matrix as a product of two low-rank matrices, which can be thought of as user and item embeddings. One of the main advantages of using MF in movie recommender systems is that it can handle large and sparse datasets efficiently.\n",
        "\n",
        "Another advantage of using MF is that it can handle cold-start and new-item problems. Cold-start refers to the scenario where there is no information available about a new user or a new item, making it difficult to make recommendations. MF can handle cold-start problems by using the low-rank embeddings to generalize across similar users or items. Similarly, when a new item is added to the system, MF can use the existing user and item embeddings to make predictions about its rating.\n",
        "\n",
        "Overall, MF is a powerful approach for building movie recommender systems because it can handle large and sparse datasets efficiently, can handle cold-start and new-item problems, and can learn low-dimensional representations that capture the underlying structure of the user-item interaction matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "xVQQDX52Lsnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps that we are going to follow in this approach are the following:\n",
        "- Map user ID to a \"user vector\" via an embedding matrix\n",
        "- Map movie ID to a \"movie vector\" via an embedding matrix\n",
        "- Compute the dot product between the user vector and movie vector, to obtain the a match score between the user and the movie (predicted rating).\n",
        "- Train the embeddings via gradient descent using all known user-movie pairs."
      ],
      "metadata": {
        "id": "M144eVIUJ7uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "FjneJBZ-1tHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Preprocessing</b>"
      ],
      "metadata": {
        "id": "WzDROTR5II3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, need to perform some preprocessing to encode users and movies as integer indices."
      ],
      "metadata": {
        "id": "Y392MqlsIIBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids = data[\"user_id\"].unique().tolist()\n",
        "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
        "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
        "movie_ids = data[\"movie_id\"].unique().tolist()\n",
        "movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n",
        "movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n",
        "data[\"user\"] = data[\"user_id\"].map(user2user_encoded)\n",
        "data[\"movie\"] = data[\"movie_id\"].map(movie2movie_encoded)\n",
        "\n",
        "num_users = len(user2user_encoded)\n",
        "num_movies = len(movie_encoded2movie)\n",
        "data[\"rating\"] = data[\"rating\"].values.astype(np.float32)\n",
        "# min and max ratings will be used to normalize the ratings later\n",
        "min_rating = min(data[\"rating\"])\n",
        "max_rating = max(data[\"rating\"])\n",
        "\n",
        "print(\n",
        "    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n",
        "        num_users, num_movies, min_rating, max_rating\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "CgvqK5I02Q4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "hOeeoIfBICFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Loss Function</b>"
      ],
      "metadata": {
        "id": "L4GiXYT2C5k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Personalized Ranking (BPR) is a loss function that is commonly used in recommendation systems and more specifically a pairwise learning-to-rank algorithm that aims to learn a personalized ranking model, that can accurately predict the preferences of a user for items based on their interactions with the system.\n",
        "\n",
        "The BPR loss function is useful in movie recommender systems because it takes into account not only the positive feedback (i.e., user ratings) but also the negative feedback (i.e., items that were not rated by the user). This is important because in many recommendation scenarios, users may only interact with a small subset of the available items, and negative feedback can be a valuable source of information for learning personalized user preferences.\n",
        "\n",
        "The BPR loss function is based on the assumption that a user prefers items that they have rated higher than items they have not rated. The bpr_loss function takes two input arguments: y_true and y_pred, which represent the true and predicted values for the pairwise preferences. The function calculates the pairwise difference matrix by taking the transpose of y_pred and subtracting it from y_pred. This matrix contains the differences between the predicted values for each pair of items. A mask is created to only consider pairs where the true value for the first item in the pair is greater than the true value for the second item in the pair. This mask is created by applying the greater function to the pairwise difference matrix. The logarithm of the sigmoid of the pairwise differences is calculated using the log_sigmoid function from TensorFlow's math module. This gives a measure of how likely the model is to predict the first item as being preferred over the second item. The mask is applied to the pairwise log sigmoid matrix by element-wise multiplication. This effectively removes the terms where the true preference for the second item is greater than the true preference for the first item. The loss is calculated as the mean of the masked pairwise log sigmoid. This is divided by the sum of the mask to ensure that only the relevant terms are included in the loss calculation. Finally, the loss value is returned as the output of the bpr_loss function."
      ],
      "metadata": {
        "id": "2aLAuQm8mvzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(y_true, y_pred):\n",
        "    # Reshape y_true and y_pred to match shape (batch_size, 1)\n",
        "    y_true = tf.reshape(y_true, (-1, 1))\n",
        "    y_pred = tf.reshape(y_pred, (-1, 1))\n",
        "\n",
        "    # Calculate the pairwise difference matrix\n",
        "    pairwise_diff = tf.transpose(y_pred) - y_pred\n",
        "\n",
        "    # Create a mask to only consider pairs where y_true[i] > y_true[j]\n",
        "    mask = tf.cast(tf.greater(pairwise_diff, 0), dtype=tf.float32)\n",
        "\n",
        "    # Calculate the log of the sigmoid of the pairwise differences\n",
        "    pairwise_log_sigmoid = -tf.math.log_sigmoid(pairwise_diff)\n",
        "\n",
        "    # Apply the mask to the pairwise log sigmoid\n",
        "    masked_pairwise_log_sigmoid = tf.multiply(pairwise_log_sigmoid, mask)\n",
        "\n",
        "    # Calculate the mean of the masked pairwise log sigmoid\n",
        "    loss = tf.reduce_sum(masked_pairwise_log_sigmoid) / tf.reduce_sum(mask)\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "OZwPPFZ1e8qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Other Loss Functions that we used but gave us worse results are: MSE, Cross-Entropy, LamdaLoss."
      ],
      "metadata": {
        "id": "sto7dSKfGrdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>MF Model</b>"
      ],
      "metadata": {
        "id": "BnHv9RtkKQem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We embed both users and movies in to 20-dimensional vectors."
      ],
      "metadata": {
        "id": "CzxUtknFKWi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model computes a match score between user and movie embeddings via a dot product, and adds a per-movie and per-user bias."
      ],
      "metadata": {
        "id": "0QqSPSOUKZ6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 20\n",
        "\n",
        "class RecommenderNetV(keras.Model):\n",
        "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
        "        super(RecommenderNetV, self).__init__(**kwargs)\n",
        "        self.num_users = num_users\n",
        "        self.num_movies = num_movies\n",
        "        self.embedding_size = embedding_size\n",
        "        self.user_movie_embedding = layers.Embedding(\n",
        "            num_users+num_movies,\n",
        "            embedding_size,\n",
        "            embeddings_initializer=\"he_normal\",\n",
        "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "        )\n",
        "        self.user_bias = layers.Embedding(num_users, 1)\n",
        "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_vector = self.user_movie_embedding(inputs[:, 0])\n",
        "        movie_vector = self.user_movie_embedding(inputs[:, 1]+num_users)\n",
        "        user_bias = self.user_bias(inputs[:, 0])\n",
        "        movie_bias = self.movie_bias(inputs[:, 1])\n",
        "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
        "        # Add all the components (including bias)\n",
        "        x = dot_user_movie + user_bias + movie_bias\n",
        "        return x\n",
        "\n",
        "\n",
        "mf_model = RecommenderNetV(num_users, num_movies, EMBEDDING_SIZE)\n",
        "mf_model.compile(\n",
        "    loss=bpr_loss,#tf.keras.losses.MeanSquaredError(),#tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.005)\n",
        ")"
      ],
      "metadata": {
        "id": "0vIvbwRK1m8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Split Training and Validation Data</b>"
      ],
      "metadata": {
        "id": "cO7wmB1UKmFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val = train_test_split(data, test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "id": "xIuYnAbVN9X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train[[\"user\", \"movie\"]].values\n",
        "y_train = train[\"rating\"].values\n",
        "\n",
        "x_val = val[[\"user\", \"movie\"]].values\n",
        "y_val = val[\"rating\"].values"
      ],
      "metadata": {
        "id": "Eggvi3QE21O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Train and Evaluate MF model</b>"
      ],
      "metadata": {
        "id": "SXp87-GvKt2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mf_history = mf_model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=128,\n",
        "    epochs=200,\n",
        "    verbose=1,\n",
        "    validation_data=(x_val, y_val),\n",
        ")"
      ],
      "metadata": {
        "id": "Rq3JOpfM2sUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Top Recommendations Function</b>"
      ],
      "metadata": {
        "id": "as-8yVsxK-go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_recomendations(model_object, user_id, train, movie_df,at = 5):\n",
        "    movies_watched_by_user = train[train.user_id == user_id]\n",
        "\n",
        "    movies_not_watched = movie_df[~movie_df[\"movie_id\"].isin(movies_watched_by_user.movie_id.values)][\"movie_id\"]\n",
        "    movies_not_watched = list(set(movies_not_watched).intersection(set(movie2movie_encoded.keys())))\n",
        "    movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n",
        "\n",
        "    \n",
        "    user_encoder = user2user_encoded.get(user_id)\n",
        "    user_movie_array = np.hstack(([[user_encoder]] * len(movies_not_watched), movies_not_watched))\n",
        "\n",
        "    ratings = model_object.predict(user_movie_array,verbose=0).flatten()\n",
        "    top_ratings_indices = ratings.argsort()[-at:][::-1]\n",
        "    recommended_movie_ids = [movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices]\n",
        "    return recommended_movie_ids\n",
        "    \n",
        "r = top_recomendations(mf_model, 4,train,data,10)\n",
        "print(r) "
      ],
      "metadata": {
        "id": "IQIQ0_KRO4F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Evaluation Metrics</b>"
      ],
      "metadata": {
        "id": "bg3gGip1K4EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(recommended_items, relevant_items):\n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
        "    \n",
        "    return precision_score\n",
        "\n",
        "def recall(recommended_items, relevant_items):  \n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
        "    \n",
        "    return recall_score\n",
        "\n",
        "def AP(recommended_items, relevant_items):\n",
        "   \n",
        "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
        "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
        "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
        "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
        "\n",
        "    return ap_score\n",
        "\n",
        "def evaluate_algorithm_top(train, test, recommender_object, movie_df, at=25, thr_relevant = 0.85):\n",
        "    \n",
        "    cumulative_precision = 0.0\n",
        "    cumulative_recall = 0.0\n",
        "    cumulative_AP = 0.0\n",
        "    \n",
        "    num_eval = 0\n",
        "\n",
        "\n",
        "    for user_id in tqdm(test.user_id.unique()):\n",
        "        \n",
        "        relevant_items = test[test.user_id==user_id]\n",
        "        thr = np.quantile(relevant_items.rating,thr_relevant)\n",
        "        relevant_items = np.array(relevant_items[relevant_items.rating >=thr].movie_id.values)\n",
        "        if len(relevant_items)>0:\n",
        "            \n",
        "            recommended_items = top_recomendations(recommender_object,user_id, train, movie_df, at=at)\n",
        "            num_eval+=1\n",
        "\n",
        "            cumulative_precision += precision(recommended_items, relevant_items)\n",
        "            cumulative_recall += recall(recommended_items, relevant_items)\n",
        "            cumulative_AP += AP(recommended_items, relevant_items)\n",
        "            \n",
        "    cumulative_precision /= num_eval\n",
        "    cumulative_recall /= num_eval\n",
        "    MAP = cumulative_AP / num_eval\n",
        "    \n",
        "    print(\"Recommender results are: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
        "        cumulative_precision, cumulative_recall, MAP)) "
      ],
      "metadata": {
        "id": "co0FRnghO0Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_algorithm_top(train, val, mf_model, data, at = 25)"
      ],
      "metadata": {
        "id": "_d1o7v1HOixj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Export final csv File</b>"
      ],
      "metadata": {
        "id": "Qre0RK3kLFzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('kaggle_baseline.csv')"
      ],
      "metadata": {
        "id": "FvHWnH6MXf7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# open the file in the write mode\n",
        "with open('solution_5.csv', 'w',encoding='UTF8') as f:\n",
        "    # create the csv writer\n",
        "    writer = csv.writer(f)\n",
        "    # write a row to the csv file\n",
        "    writer.writerow(['user_id', 'prediction'])\n",
        "    for user_id in tqdm(test.user_id.unique()):\n",
        "      try:\n",
        "          relevant_items = top_recomendations(mf_model, user_id,train,data,25)\n",
        "          list_relevants = ' '.join([str(elem) for elem in relevant_items])\n",
        "          writer.writerow([str(user_id),list_relevants])\n",
        "      except KeyError as e:\n",
        "          print(f\"Error: {e}. User ID {user_id} not found.\")\n"
      ],
      "metadata": {
        "id": "W6bRFlTNz5kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using this method we had the following scores using different loss functions:\n",
        "- 0.02496 when using MSE loss function\n",
        "- 0.04864 when using BPR loss function\n",
        "\n",
        "BPR Gave us the best result."
      ],
      "metadata": {
        "id": "cVOI9XCMvLdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have also tried to use different learning rate values to get better results:\n",
        "\n",
        "- 0.03\n",
        "- 0.01\n",
        "- 0.005\n",
        "- 0.001"
      ],
      "metadata": {
        "id": "UgeLfYPMXo5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We increased the Embedding size to see if we will get a better MAP score. We tried different values (10, 20, 25, 30, 35)."
      ],
      "metadata": {
        "id": "U4x4w2HEygdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also tried different Optimizers: Adam, Adagrad"
      ],
      "metadata": {
        "id": "Cx9_aDvGCpL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eventually, the best MAP score that we 've got is <b>0.04864</b>."
      ],
      "metadata": {
        "id": "-MSBVff1Ha4v"
      }
    }
  ]
}