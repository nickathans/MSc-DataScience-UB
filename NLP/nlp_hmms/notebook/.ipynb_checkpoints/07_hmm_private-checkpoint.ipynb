{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Basic algorithm for sequence models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Hidden markov models learning\n",
    "\n",
    "- Computing counts from a corpus\n",
    "- From counts to probabilities: finding the parameters of initial, transition and emission probabilities.\n",
    "- Smoothing non seen events in avoid overfitting.\n",
    "\n",
    "##  Working with scores instead of probabilities\n",
    "\n",
    "- Rewritting probabilities with log probabilities. \n",
    "- Why do we want to work with log probabilities? Understand issues when computing a product of probabilities.\n",
    "- The `logsumexp` trick.\n",
    "\n",
    "##  The forward backward algorithm\n",
    "\n",
    "- Computing the probability of a sequence $P(X=x)$\n",
    "- Computing the probability of a stage given an input sequence $P(Y_i=x \\,\\vert\\, X=x)$. \n",
    "- Use this probabilities to do Posterior decoding. Evaluate posterior decoding in a problem.\n",
    "\n",
    "\n",
    "## Computing the most likely hidden state sequence: The viterbi algorithm\n",
    "\n",
    "\n",
    "- How to compute $argmax_{y_1,\\dots,y_N \\in \\Lambda^N} P(Y_{1:N}=y_{1:N}\\,\\vert\\, X=x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:07:57.060604Z",
     "start_time": "2022-04-05T05:07:57.055139Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import os,sys,inspect\n",
    "#currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "#parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,'../') \n",
    "import skseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classes used to store the sequences\n",
    "\n",
    "We will use\n",
    "\n",
    "- class ``Sequence`` in ``skseq/sequences/sequence.py`` file\n",
    "- class ``LabelDictionary`` in ```skseq/sequences/label_dictionary.py`` file\n",
    "- class ``SequenceList`` in ```skseq/sequences/sequence_list.py`` file\n",
    "- class ``_SequenceIterator`` in ```skseq/sequences/sequence_list.py`` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a Sequence Object\n",
    "\n",
    "- Sequence objects are defined in ```skseq/sequences/sequence.py``. \n",
    "    - A sequence in a supervised learning problem consist on a set of words and tags associated to words.\n",
    "    - For example ``w_1/t_1 w_2/t_2 w_3/t_3`` is a sequence of lenght 3 with words ``w_i`` and tags ``t_i``.\n",
    "\n",
    "\n",
    "In order to instanciate a Sequence we essentially need a list of words and a list of tags of the same size. In order to do it efficiently we will not store strings for the words and tags. We will store an integer values that will represent words and tags.\n",
    "\n",
    "- **.x** attribute: list of words (integer words)\n",
    "\n",
    "- **.y** attribute: list of tags (integer tags)\n",
    "\n",
    "Then we need to keep a mapping from integers to words and from integers to tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:08:01.697974Z",
     "start_time": "2022-04-05T05:08:01.692355Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skseq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-28587cfad295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skseq'"
     ]
    }
   ],
   "source": [
    "import skseq\n",
    "import skseq.sequences\n",
    "import skseq.readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:08:02.261594Z",
     "start_time": "2022-04-05T05:08:02.257520Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skseq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ae5aaaeef8b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m seq = skseq.sequences.sequence.Sequence(x=[1,3,2,4],\n\u001b[1;32m      4\u001b[0m                                             y=[0,2,1,1])\n\u001b[1;32m      5\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skseq'"
     ]
    }
   ],
   "source": [
    "from skseq.sequences import sequence\n",
    "\n",
    "seq = skseq.sequences.sequence.Sequence(x=[1,3,2,4],\n",
    "                                            y=[0,2,1,1])\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:08:03.001384Z",
     "start_time": "2022-04-05T05:08:02.996472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "David/NounEntity was/0 happy/0 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = skseq.sequences.sequence.Sequence(x=[\"David\",\"was\",\"happy\"],\n",
    "                                            y=[\"NounEntity\",\"0\",\"0\"])\n",
    "\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a vocabulary and a SequenceList\n",
    "\n",
    "Given a training set with words and tags we want to build a SequenceList object definded in  ``skseq/sequences/sequence_list.py``.\n",
    "\n",
    "A  SequenceList is a class that is initialized using a\n",
    "- dictionary for the words\n",
    "- a dictionary for the tags\n",
    "- an empty sequence list where the Sequences read from the data will be stored.\n",
    "\n",
    "\n",
    "    class SequenceList(object):\n",
    "\n",
    "        def __init__(self, x_dict, y_dict):\n",
    "            self.x_dict = x_dict\n",
    "            self.y_dict = y_dict\n",
    "            self.seq_list = []\n",
    "\n",
    "\n",
    "Let us create 3 sequence list for train, test and validation.  \n",
    "\n",
    "We will use the conll dataset and the class  ``PostagCorpus``.\n",
    "The class has a method ``.read_sequence_list_conll`` that will return the **SequenceList** object we want\n",
    "\n",
    "\n",
    "\n",
    "    def read_sequence_list_conll(self, train_file,\n",
    "                                 mapping_file=(\"%s/en-ptb.map\"\n",
    "                                               % dirname(__file__)),\n",
    "                                 max_sent_len=100000,\n",
    "                                 max_nr_sent=100000):\n",
    "\n",
    "        # Build mapping of postags:\n",
    "        mapping = {}\n",
    "        if mapping_file is not None:\n",
    "            for line in open(mapping_file):\n",
    "                coarse, fine = line.strip().split(\"\\t\")\n",
    "                mapping[coarse.lower()] = fine.lower()\n",
    "\n",
    "        instance_list = self.read_conll_instances(train_file,\n",
    "                                                  max_sent_len,\n",
    "                                                  max_nr_sent,\n",
    "                                                  mapping)\n",
    "\n",
    "        seq_list = SequenceList(self.word_dict, self.tag_dict)\n",
    "\n",
    "        for sent_x, sent_y in instance_list:\n",
    "            seq_list.add_sequence(sent_x, sent_y,  self.word_dict, self.tag_dict)\n",
    "\n",
    "        return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:13:01.693871Z",
     "start_time": "2022-04-05T05:13:01.575802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev-22.conll      test-23.conll     train-02-21.conll\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:14:26.868601Z",
     "start_time": "2022-04-05T05:14:26.753503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tIn\t_\tIN\tIN\t_\t43\tVMOD\t_\t_\r\n",
      "2\tan\t_\tDT\tDT\t_\t5\tNMOD\t_\t_\r\n",
      "3\tOct.\t_\tNN\tNNP\t_\t5\tNMOD\t_\t_\r\n",
      "4\t19\t_\tCD\tCD\t_\t5\tNMOD\t_\t_\r\n",
      "5\treview\t_\tNN\tNN\t_\t1\tPMOD\t_\t_\r\n",
      "6\tof\t_\tIN\tIN\t_\t5\tNMOD\t_\t_\r\n",
      "7\t``\t_\t.\t``\t_\t9\tP\t_\t_\r\n",
      "8\tThe\t_\tDT\tDT\t_\t9\tNMOD\t_\t_\r\n",
      "9\tMisanthrope\t_\tNN\tNN\t_\t6\tPMOD\t_\t_\r\n",
      "10\t''\t_\t.\t''\t_\t9\tP\t_\t_\r\n",
      "11\tat\t_\tIN\tIN\t_\t9\tNMOD\t_\t_\r\n",
      "12\tChicago\t_\tNN\tNNP\t_\t13\tNMOD\t_\t_\r\n",
      "13\t's\t_\tPO\tPOS\t_\t15\tNMOD\t_\t_\r\n",
      "14\tGoodman\t_\tNN\tNNP\t_\t15\tNMOD\t_\t_\r\n",
      "15\tTheatre\t_\tNN\tNNP\t_\t11\tPMOD\t_\t_\r\n",
      "16\t(\t_\t.\t-LRB-\t_\t30\tP\t_\t_\r\n",
      "17\t``\t_\t.\t``\t_\t30\tP\t_\t_\r\n",
      "18\tRevitalized\t_\tVB\tVBN\t_\t19\tNMOD\t_\t_\r\n",
      "19\tClassics\t_\tNN\tNNS\t_\t20\tSUB\t_\t_\r\n",
      "20\tTake\t_\tVB\tVBP\t_\t30\tDEP\t_\t_\r\n",
      "21\tthe\t_\tDT\tDT\t_\t22\tNMOD\t_\t_\r\n",
      "22\tStage\t_\tNN\tNN\t_\t20\tOBJ\t_\t_\r\n",
      "23\tin\t_\tIN\tIN\t_\t20\tVMOD\t_\t_\r\n",
      "24\tWindy\t_\tNN\tNNP\t_\t25\tNMOD\t_\t_\r\n",
      "25\tCity\t_\tNN\tNNP\t_\t23\tPMOD\t_\t_\r\n",
      "26\t,\t_\t.\t,\t_\t30\tP\t_\t_\r\n",
      "27\t''\t_\t.\t''\t_\t30\tP\t_\t_\r\n",
      "28\tLeisure\t_\tNN\tNN\t_\t30\tNMOD\t_\t_\r\n",
      "29\t&\t_\tCC\tCC\t_\t30\tNMOD\t_\t_\r\n",
      "30\tArts\t_\tNN\tNNS\t_\t5\tNMOD\t_\t_\r\n",
      "31\t)\t_\t.\t-RRB-\t_\t30\tP\t_\t_\r\n",
      "32\t,\t_\t.\t,\t_\t43\tP\t_\t_\r\n",
      "33\tthe\t_\tDT\tDT\t_\t34\tNMOD\t_\t_\r\n",
      "34\trole\t_\tNN\tNN\t_\t43\tSUB\t_\t_\r\n",
      "35\tof\t_\tIN\tIN\t_\t34\tNMOD\t_\t_\r\n",
      "36\tCelimene\t_\tNN\tNNP\t_\t35\tPMOD\t_\t_\r\n",
      "37\t,\t_\t.\t,\t_\t34\tP\t_\t_\r\n",
      "38\tplayed\t_\tVB\tVBN\t_\t34\tNMOD\t_\t_\r\n",
      "39\tby\t_\tIN\tIN\t_\t38\tVMOD\t_\t_\r\n",
      "40\tKim\t_\tNN\tNNP\t_\t41\tNMOD\t_\t_\r\n",
      "41\tCattrall\t_\tNN\tNNP\t_\t39\tPMOD\t_\t_\r\n",
      "42\t,\t_\t.\t,\t_\t34\tP\t_\t_\r\n",
      "43\twas\t_\tVB\tVBD\t_\t0\tROOT\t_\t_\r\n",
      "44\tmistakenly\t_\tRB\tRB\t_\t45\tVMOD\t_\t_\r\n",
      "45\tattributed\t_\tVB\tVBN\t_\t43\tVC\t_\t_\r\n",
      "46\tto\t_\tTO\tTO\t_\t45\tVMOD\t_\t_\r\n",
      "47\tChristina\t_\tNN\tNNP\t_\t48\tNMOD\t_\t_\r\n",
      "48\tHaag\t_\tNN\tNNP\t_\t46\tPMOD\t_\t_\r\n",
      "49\t.\t_\t.\t.\t_\t43\tP\t_\t_\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 ../data/conll/train-02-21.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:14.414211Z",
     "start_time": "2022-04-05T05:09:13.907844Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skseq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-201ce6c922cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPostagCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/conll\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skseq'"
     ]
    }
   ],
   "source": [
    "import skseq.readers.pos_corpus\n",
    "corpus = skseq.readers.pos_corpus.PostagCorpus()\n",
    "\n",
    "data_path = \"../data/conll\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=100, max_nr_sent=5000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"/test-23.conll\",\n",
    "                                           max_sent_len=100, max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"/dev-22.conll\", \n",
    "                                          max_sent_len=100, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:14.479436Z",
     "start_time": "2022-04-05T05:09:14.476106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skseq.sequences.sequence.Sequence"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:14.949002Z",
     "start_time": "2022-04-05T05:09:14.946274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skseq.sequences.sequence_list.SequenceList"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:15.311558Z",
     "start_time": "2022-04-05T05:09:15.308503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_dict', 'y_dict', 'seq_list'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:15.694722Z",
     "start_time": "2022-04-05T05:09:15.691437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16937, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq.x_dict), len(train_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:16.386245Z",
     "start_time": "2022-04-05T05:09:16.383331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_dict', 'y_dict', 'seq_list'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:16.746369Z",
     "start_time": "2022-04-05T05:09:16.743120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16937, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_seq.x_dict), len(test_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:17.960410Z",
     "start_time": "2022-04-05T05:09:17.957567Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:18.680471Z",
     "start_time": "2022-04-05T05:09:18.677614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence\n",
    "train_seq[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:19.063648Z",
     "start_time": "2022-04-05T05:09:19.060706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:20.304586Z",
     "start_time": "2022-04-05T05:09:20.301491Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_dict', 'y_dict', 'seq_list'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:21.055316Z",
     "start_time": "2022-04-05T05:09:21.052146Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adp': 0,\n",
       " 'det': 1,\n",
       " 'noun': 2,\n",
       " 'num': 3,\n",
       " '.': 4,\n",
       " 'prt': 5,\n",
       " 'verb': 6,\n",
       " 'conj': 7,\n",
       " 'adv': 8,\n",
       " 'pron': 9,\n",
       " 'adj': 10,\n",
       " 'x': 11}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of possible tags Lambda\n",
    "# train_seq.y_dict is a dictionary of mappings from tag to integer id \n",
    "train_seq.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:21.886875Z",
     "start_time": "2022-04-05T05:09:21.883778Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16937"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of possible words\n",
    "len(train_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:22.499405Z",
     "start_time": "2022-04-05T05:09:22.496272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our corpus ``sequencelist`` to map integers to words\n",
    "\n",
    "Sequences can use ``SequenceList`` objects to map word_ids and tag_ids to words and tags.\n",
    "\n",
    "All ``sequence`` objects have the **``.to_words``** method which allows us to print the words given a **``SequenceList``** object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:23.890340Z",
     "start_time": "2022-04-05T05:09:23.887078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_dict', 'y_dict', 'seq_list'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:24.488133Z",
     "start_time": "2022-04-05T05:09:24.486048Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence = train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:24.938297Z",
     "start_time": "2022-04-05T05:09:24.935492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:25.637377Z",
     "start_time": "2022-04-05T05:09:25.634379Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ms./noun Haag/noun plays/verb Elianti/noun ./. '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.to_words(sequence_list=train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Hidden markov models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:27.950128Z",
     "start_time": "2022-04-05T05:09:27.948063Z"
    }
   },
   "outputs": [],
   "source": [
    "import skseq.sequences.sequence as seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:28.693545Z",
     "start_time": "2022-04-05T05:09:28.691300Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = [\"walk\", \"shop\", \"clean\", \"tennis\"]\n",
    "Lambda = [\"rainy\", \"sunny\"]\n",
    "\n",
    "sequence_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:29.019665Z",
     "start_time": "2022-04-05T05:09:29.015886Z"
    }
   },
   "outputs": [],
   "source": [
    "# walk/rainy walk/sunny shop/sunny clean/sunny \n",
    "# walk/rainy walk/rainy shop/rainy clean/sunny \n",
    "# walk/sunny shop/sunny shop/sunny clean/sunny \n",
    "\n",
    "s1 = seq.Sequence([\"walk\", \"walk\", \"shop\", \"clean\"],\n",
    "                  [\"rainy\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "s2 = seq.Sequence([\"walk\", \"walk\", \"shop\", \"clean\"], \n",
    "                  [\"rainy\", \"rainy\", \"rainy\", \"sunny\"])\n",
    "\n",
    "s3 = seq.Sequence([\"walk\", \"shop\", \"shop\", \"clean\"], \n",
    "                  [\"sunny\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "train_sequences = [s1, s2, s3];\n",
    "\n",
    "s1_t = seq.Sequence([\"walk\", \"walk\", \"shop\", \"clean\"], \n",
    "                    [\"rainy\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "s2_t = seq.Sequence([\"clean\", \"walk\", \"tennis\", \"walk\"], \n",
    "                    [\"sunny\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "test_sequences = [s1_t, s2_t];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:29.332353Z",
     "start_time": "2022-04-05T05:09:29.329239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk/rainy walk/sunny shop/sunny clean/sunny "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:30.121087Z",
     "start_time": "2022-04-05T05:09:30.119135Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_pos  = {\"walk\":0,  \"shop\":1, \"clean\":2, \"tennis\":3}\n",
    "state_to_pos = {\"rainy\":0, \"sunny\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:30.837622Z",
     "start_time": "2022-04-05T05:09:30.833665Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_initial_counts(initial_counts, seq, state_to_pos):\n",
    "    \n",
    "    initial_counts[state_to_pos[seq.y[0]]] +=  1\n",
    "\n",
    "\n",
    "def update_transition_counts(transition_counts, seq, state_to_pos):\n",
    "\n",
    "    \n",
    "    #for (t_prev,t) in zip(seq.y[:-1], seq.y[1:]):\n",
    "    #    transition_counts[state_to_pos[t_prev], state_to_pos[t]] += 1 \n",
    "    #array([[2., 2.],\n",
    "    #       [0., 5.]])\n",
    "    \n",
    "    for (t_prev,t) in zip(seq.y[:-1], seq.y[1:]):\n",
    "        transition_counts[state_to_pos[t], state_to_pos[t_prev]] += 1 \n",
    "    # array([[2., 0.],\n",
    "    #        [2., 5.]])\n",
    "\n",
    "        \n",
    "def update_emission_counts(emission_counts, seq, state_to_pos, word_to_pos):\n",
    "    \n",
    "    for (t,w) in zip(seq.y, seq.x):\n",
    "        emission_counts[state_to_pos[t], word_to_pos[w]] += 1 \n",
    "        \n",
    "\n",
    "def update_final_counts(final_counts, seq, state_to_pos):\n",
    "    \n",
    "    final_counts[state_to_pos[seq.y[-1]]] +=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the previous function we can train the emisssion, initial and transition probabilities by simply counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:31.776510Z",
     "start_time": "2022-04-05T05:09:31.773001Z"
    }
   },
   "outputs": [],
   "source": [
    "def sufficient_statistics_hmm(sequences, state_to_pos, word_to_pos):\n",
    "    \n",
    "    n_states = len(state_to_pos)\n",
    "    n_words  = len(word_to_pos)\n",
    "    initial_counts      = np.zeros((n_states))\n",
    "    transition_counts   = np.zeros((n_states, n_states))\n",
    "    final_counts        = np.zeros((n_states))\n",
    "    emission_counts     = np.zeros((n_states, n_words))\n",
    "    \n",
    "    for seq in sequences:\n",
    "        update_initial_counts(initial_counts, seq, state_to_pos)\n",
    "        update_transition_counts(transition_counts, seq,  state_to_pos)\n",
    "        update_emission_counts(emission_counts, seq,  state_to_pos, word_to_pos) \n",
    "        update_final_counts(final_counts, seq,  state_to_pos) \n",
    "    \n",
    "    return initial_counts, transition_counts, final_counts, emission_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:32.117708Z",
     "start_time": "2022-04-05T05:09:32.115580Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = sufficient_statistics_hmm(train_sequences, \n",
    "                                   state_to_pos,\n",
    "                                   word_to_pos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:32.501630Z",
     "start_time": "2022-04-05T05:09:32.499512Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_counts, transition_counts, final_counts, emission_counts = counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:33.060629Z",
     "start_time": "2022-04-05T05:09:33.057336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:33.739052Z",
     "start_time": "2022-04-05T05:09:33.735940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [2., 5.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:34.098639Z",
     "start_time": "2022-04-05T05:09:34.095480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:34.482318Z",
     "start_time": "2022-04-05T05:09:34.478856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 2.],\n",
       "       [1., 3.],\n",
       "       [0., 3.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:34.802593Z",
     "start_time": "2022-04-05T05:09:34.799687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walk': 0, 'shop': 1, 'clean': 2, 'tennis': 3}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:35.114109Z",
     "start_time": "2022-04-05T05:09:35.111345Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rainy': 0, 'sunny': 1}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_to_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Checks HMM\n",
    "\n",
    "- Initial counts must sum to the number of sentences  $$ \\sum_{k=1}^K C_{\\text{init}}(c_k) = M$$\n",
    "\n",
    "- Transition counts and Final Counts should sum to the number of tokens: $$\\sum_{k,l=1}^K C_{\\text{trans}}(c_k,c_l)  + \\sum_{k=1}^K C_{\\text{final}}(c_k) = M \\cdot N$$\n",
    "\n",
    "- Emission counts must sum to the number of tokens\n",
    "$$\n",
    "\\sum_{j=1}^J \\sum_{k=1}^K C_{\\text{emiss}}(w_j,c_k) = M \\cdot N \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:37.671334Z",
     "start_time": "2022-04-05T05:09:37.668540Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\t 3 \n",
      "N:\t 4 \n",
      "M*N:\t 12\n"
     ]
    }
   ],
   "source": [
    "M = len(train_sequences)\n",
    "N = len(train_sequences[0].x)\n",
    "print(\"M:\\t\", M, \"\\nN:\\t\", N,\"\\nM*N:\\t\", M*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:40.259942Z",
     "start_time": "2022-04-05T05:09:40.257021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_counts sum:  3.0\n",
      "emission_counts sum:  12.0\n",
      "transition and final counts sum:  12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"initial_counts sum: \", np.sum(initial_counts))\n",
    "print(\"emission_counts sum: \", np.sum(emission_counts))\n",
    "print(\"transition and final counts sum: \",\\\n",
    "       np.sum(transition_counts) + sum(final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:41.316976Z",
     "start_time": "2022-04-05T05:09:41.313751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:41.907727Z",
     "start_time": "2022-04-05T05:09:41.904596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [2., 5.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:42.238551Z",
     "start_time": "2022-04-05T05:09:42.235317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:42.569620Z",
     "start_time": "2022-04-05T05:09:42.566536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 1., 0., 0.],\n",
       "       [2., 3., 3., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From counts to probabilities\n",
    "\n",
    "The following formulas specify how to find the parameters of the HMM:\n",
    "\n",
    "$$\n",
    "P_{\\text{init}}(c_k \\,\\vert\\, \\text{start}) = \\frac{C_{\\text{init}}(c_k)}{ \\sum_{k=1}^K\n",
    "C_{\\text{init}} (c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{final}}(\\text{stop} \\,\\vert\\, c_l) = \\frac{C_{\\text{final}}(c_l) }\n",
    "{\\sum_{k=1}^K C_{\\text{trans}}(c_k,c_l) + C_{\\text{final}}(c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{trans}}( c_k \\,\\vert\\, c_l) = \\frac{C_{\\text{trans}}(c_k, c_l) }\n",
    "{\\sum_{p=1}^K C_{\\text{trans}}(c_p,c_l) + C_{\\text{final}}(c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{emiss}} (w_j \\,\\vert\\, c_k) = \\frac{C_{\\text{emiss}} (w_j, c_k) }{\\sum_{q=1}^J C_{\\text{emiss}}(w_q,c_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:43.298823Z",
     "start_time": "2022-04-05T05:09:43.294236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "initial_probs\n",
      "[0.66666667 0.33333333]\n",
      "\n",
      "transition_probs\n",
      "[[0.5   0.   ]\n",
      " [0.5   0.625]]\n",
      "\n",
      "final_probs\n",
      "[0.    0.375]\n",
      "\n",
      "emission_probs\n",
      "[[0.75  0.25 ]\n",
      " [0.25  0.375]\n",
      " [0.    0.375]\n",
      " [0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "#initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "#transition_probs = transition_counts/(np.sum(transition_counts, 1, keepdims=True) + final_counts)\n",
    "#final_probs      = final_counts/(np.sum(transition_counts, 1, keepdims=True) + final_counts )\n",
    "#emission_probs   = emission_counts / np.sum(emission_counts, 1, keepdims=True)\n",
    "\n",
    "initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "transition_probs = transition_counts/(np.sum(transition_counts,0) + final_counts)\n",
    "final_probs      = final_counts/(np.sum(transition_counts, 0) + final_counts )\n",
    "emission_probs   = emission_counts.T / np.sum(emission_counts, 1)\n",
    "\n",
    "print(\"\\ninitial_probs\")\n",
    "print(initial_probs)\n",
    "\n",
    "print(\"\\ntransition_probs\")\n",
    "print(transition_probs)\n",
    "\n",
    "print(\"\\nfinal_probs\")\n",
    "print(final_probs)\n",
    "\n",
    "print(\"\\nemission_probs\")\n",
    "print(emission_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:44.512462Z",
     "start_time": "2022-04-05T05:09:44.509305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walk': 0, 'shop': 1, 'clean': 2, 'tennis': 3}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OBSERVATION:\n",
    "\n",
    "**If we stack trainsition and final counts and normalize them we get\n",
    "a proper conditional probability distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:46.941388Z",
     "start_time": "2022-04-05T05:09:46.939289Z"
    }
   },
   "outputs": [],
   "source": [
    "transitions_with_final_counts = np.vstack((transition_counts,\n",
    "                                           final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:47.260514Z",
     "start_time": "2022-04-05T05:09:47.257201Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [2., 5.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions_with_final_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:47.844831Z",
     "start_time": "2022-04-05T05:09:47.841383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.   ],\n",
       "       [0.5  , 0.625],\n",
       "       [0.   , 0.375]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions_with_final_counts/ np.sum(transitions_with_final_counts,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with scores \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:49.542383Z",
     "start_time": "2022-04-05T05:09:49.537917Z"
    }
   },
   "outputs": [],
   "source": [
    "def logzero():\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    print(x)\n",
    "    if x == 0:\n",
    "        return logzero()\n",
    "    return np.log(x)\n",
    "\n",
    "\n",
    "def logsum_pair(logx, logy):\n",
    "    \"\"\"\n",
    "    Return log(x+y), avoiding arithmetic underflow/overflow.\n",
    "\n",
    "    logx: log(x)\n",
    "    logy: log(y)\n",
    "\n",
    "    Rationale:\n",
    "\n",
    "    x + y    = e^logx + e^logy\n",
    "             = e^logx (1 + e^(logy-logx))\n",
    "    log(x+y) = logx + log(1 + e^(logy-logx)) (1)\n",
    "\n",
    "    Likewise,\n",
    "    log(x+y) = logy + log(1 + e^(logx-logy)) (2)\n",
    "\n",
    "    The computation of the exponential overflows earlier and is less precise\n",
    "    for big values than for small values. Due to the presence of logy-logx\n",
    "    (resp. logx-logy), (1) is preferred when logx > logy and (2) is preferred\n",
    "    otherwise.\n",
    "    \"\"\"\n",
    "    if logx == logzero():\n",
    "        return logy\n",
    "    elif logx > logy:\n",
    "        return logx + np.log1p(np.exp(logy-logx))\n",
    "    else:\n",
    "        return logy + np.log1p(np.exp(logx-logy))\n",
    "\n",
    "\n",
    "def logsum(logv):\n",
    "    \"\"\"\n",
    "    Return log(v[0]+v[1]+...), avoiding arithmetic underflow/overflow.\n",
    "    \"\"\"\n",
    "    res = logzero()\n",
    "    for val in logv:\n",
    "        res = logsum_pair(res, val)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:49.876913Z",
     "start_time": "2022-04-05T05:09:49.871938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7999098843442183\n",
      "9.45876378447825\n",
      "90.00004540096037\n",
      "inf\n",
      "\n",
      "\n",
      "2.7999098843442183\n",
      "9.45876378447825\n",
      "90.00004540096037\n",
      "900.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/4272209461.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.log(sum(np.exp(1000*a))))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#a = np.random.rand([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "a = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "\n",
    "print(np.log(sum(np.exp(a))))\n",
    "print(np.log(sum(np.exp(10*a))))\n",
    "print(np.log(sum(np.exp(100*a))))\n",
    "print(np.log(sum(np.exp(1000*a))))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(logsum(a))\n",
    "print(logsum(10*a))\n",
    "print(logsum(100*a))\n",
    "print(logsum(1000*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:50.241729Z",
     "start_time": "2022-04-05T05:09:50.236244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7999098843442183\n",
      "9.45876378447825\n",
      "90.00004540096037\n",
      "inf\n",
      "\n",
      "\n",
      "2.7999098843442183\n",
      "9.45876378447825\n",
      "90.00004540096037\n",
      "900.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/2421178076.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.log(sum(np.exp(1000*a))))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logsumexp(vec):\n",
    "    c = np.max(vec)\n",
    "    return c + np.log(np.sum(np.exp(vec-c)))\n",
    "\n",
    "#a = np.random.rand([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "a = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "\n",
    "print(np.log(sum(np.exp(a))))\n",
    "print(np.log(sum(np.exp(10*a))))\n",
    "print(np.log(sum(np.exp(100*a))))\n",
    "print(np.log(sum(np.exp(1000*a))))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(logsumexp(a))\n",
    "print(logsumexp(10*a))\n",
    "print(logsumexp(100*a))\n",
    "print(logsumexp(1000*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:50.861150Z",
     "start_time": "2022-04-05T05:09:50.859158Z"
    }
   },
   "outputs": [],
   "source": [
    "# How can we find this? probabilities will be between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:52.098563Z",
     "start_time": "2022-04-05T05:09:52.052823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:152: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:152: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if decode is 'posterior':\n",
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:152: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if decode is 'viterbi':\n"
     ]
    }
   ],
   "source": [
    "class HMM(object):\n",
    "    \n",
    "    def __init__(self, word_to_pos={}, state_to_pos={}):\n",
    "        self.fitted = False\n",
    "        self.counts = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.probs  = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.scores = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.decode = set([\"posterior\", \"viterbi\"])\n",
    "        self.word_to_pos  = word_to_pos\n",
    "        self.state_to_pos = state_to_pos\n",
    "        self.pos_to_word  = {v: k for k, v in word_to_pos.items()}\n",
    "        self.pos_to_state = {v: k for k, v in state_to_pos.items()}\n",
    "    \n",
    "        self.n_states     = len(state_to_pos)\n",
    "        self.n_words      = len(word_to_pos)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, observation_lables: list, state_labels: list):\n",
    "        \"\"\"\n",
    "        Computes and saves: counts, probs, scores.\n",
    "        \"\"\"\n",
    "        if self.state_to_pos is None or self.word_to_pos is None:\n",
    "            print(\"Error state_to_pos or word_to_pos needed to be defined\")\n",
    "            return\n",
    "            \n",
    "        self.counts = self.sufficient_statistics_hmm(observation_lables, state_labels)       \n",
    "        self.probs  = self.compute_probs(self.counts)  \n",
    "        self.scores = self.compute_scores(self.probs)  \n",
    "        self.fitted = True\n",
    "        \n",
    "    def sufficient_statistics_hmm(self, observation_lables, state_labels):\n",
    "\n",
    "        state_to_pos, word_to_pos = self.state_to_pos, self.word_to_pos\n",
    "        \n",
    "        def update_initial_counts(initial_counts, seq_x, state_to_pos):\n",
    "            initial_counts[state_to_pos[seq_x[0]]] +=  1\n",
    "            \n",
    "        def update_transition_counts(transition_counts, seq_y, state_to_pos):\n",
    "            for (t_prev, t) in zip(seq_y[:-1], seq_y[1:]):\n",
    "                transition_counts[state_to_pos[t], state_to_pos[t_prev]] += 1 \n",
    "\n",
    "        def update_emission_counts(emission_counts, seq_x, seq_y, state_to_pos, word_to_pos):\n",
    "            for (t,x) in zip(seq_y, seq_x):\n",
    "                emission_counts[state_to_pos[t], word_to_pos[x]] += 1 \n",
    "                \n",
    "        def update_final_counts(final_counts, seq_y, state_to_pos):\n",
    "            final_counts[state_to_pos[seq_y[-1]]] +=1\n",
    "\n",
    "        n_states = len(state_to_pos)\n",
    "        n_words  = len(word_to_pos)\n",
    "        initial_counts      = np.zeros((n_states))\n",
    "        transition_counts   = np.zeros((n_states, n_states))\n",
    "        final_counts        = np.zeros((n_states))\n",
    "        emission_counts     = np.zeros((n_states, n_words))\n",
    "\n",
    "        for seq_x, seq_y in zip(observation_lables, state_labels):\n",
    "            update_initial_counts(initial_counts, seq_y, state_to_pos)\n",
    "            update_transition_counts(transition_counts, seq_y,  state_to_pos)\n",
    "            update_emission_counts(emission_counts, seq_x, seq_y, state_to_pos, word_to_pos) \n",
    "            update_final_counts(final_counts, seq_y,  state_to_pos) \n",
    "\n",
    "        return {\"emission\":   emission_counts, \n",
    "                \"transition\": transition_counts,\n",
    "                \"final\":      final_counts, \n",
    "                \"initial\":    initial_counts}\n",
    "    \n",
    "    def compute_probs(self, counts):\n",
    "        \n",
    "        initial_counts    = counts['initial']\n",
    "        transition_counts = counts['transition']\n",
    "        emission_counts   = counts['emission']\n",
    "        final_counts      = counts['final']\n",
    "\n",
    "        initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "        transition_probs = transition_counts/(np.sum(transition_counts,0) + final_counts)\n",
    "        final_probs      = final_counts/(np.sum(transition_counts, 0) + final_counts )\n",
    "        emission_probs   = (emission_counts.T / np.sum(emission_counts, 1)).T\n",
    "    \n",
    "        return {\"emission\":   emission_probs, \n",
    "                \"transition\": transition_probs,\n",
    "                \"final\":      final_probs, \n",
    "                \"initial\":    initial_probs}\n",
    "    \n",
    "    def compute_scores(self, probs):\n",
    "         return {\"emission\":   np.log(probs[\"emission\"]), \n",
    "                 \"transition\": np.log(probs[\"transition\"]),\n",
    "                 \"final\":      np.log(probs[\"final\"]), \n",
    "                 \"initial\":    np.log(probs[\"initial\"])}\n",
    "        \n",
    "    def forward_computations(self, x: list):\n",
    "        forward_x = None\n",
    "        return forward_x\n",
    "    \n",
    "    def backward_computations(self, x:list):\n",
    "        backward_x = None\n",
    "        return backward_x\n",
    "    \n",
    "    def log_forward_computations(self, x: list):\n",
    "        \"\"\"\n",
    "        Compute the log_forward computations\n",
    "\n",
    "        Assume there are S possible states and a sequence of length N.\n",
    "        This method will compute iteritavely the log_forward quantities.\n",
    "\n",
    "        * log_f is a S x N Array.\n",
    "        * log_f_x[:,i] will contain the forward quantities at position i.\n",
    "        * log_f_x[:,i] is a vector of size S.\n",
    "        \n",
    "        Returns\n",
    "        - log_f_x: Array of size K x N\n",
    "        \"\"\" \n",
    "        n_x = len(x)\n",
    "        \n",
    "        # log_f_x initialized to -Inf because log(0) = -Inf\n",
    "        log_f_x = np.zeros((self.n_states, n_x)) - np.Inf\n",
    "        x_emission_scores = np.array([hmm.scores['emission'][:, hmm.word_to_pos[w]] for w in x]).T\n",
    "        \n",
    "        log_f_x[:,0] = x_emission_scores[:, 0] + self.scores['initial']\n",
    "        for n in range(1, n_x):\n",
    "            for s in range(self.n_states):\n",
    "                log_f_x[s,n] = logsum(log_f_x[:,n-1] + self.scores['transition'][s,:]) + x_emission_scores[s,n]\n",
    "\n",
    "        log_likelihood = logsum(log_f_x[:,n_x-1] + self.scores['final']) \n",
    "        return log_f_x, log_likelihood\n",
    "    \n",
    "    \n",
    "    def log_backward_computations(self, x: list):\n",
    "        n_x = len(x)\n",
    "        \n",
    "        # log_f_x initialized to -Inf because log(0) = -Inf\n",
    "        log_b_x = np.zeros((self.n_states, n_x)) - np.Inf\n",
    "        x_emission_scores = np.array([hmm.scores['emission'][:, hmm.word_to_pos[w]] for w in x]).T\n",
    "        log_b_x[:,-1] = self.scores['final']\n",
    "\n",
    "        for n in range(n_x-2, -1, -1):\n",
    "            for s in range(self.n_states):\n",
    "                log_b_x[s,n] = logsum(log_b_x[:,n+1] + self.scores['transition'][:,s] + x_emission_scores[:,n+1])\n",
    "\n",
    "        log_likelihood = logsum(log_b_x[:,0] + self.scores['initial'] + x_emission_scores[:,0]) \n",
    "        return log_b_x, log_likelihood\n",
    "        \n",
    "    def predict_labels(self, x: list, decode=\"posterior\"):\n",
    "        \"\"\"\n",
    "        Retuns a sequence of states for each word in **x**.\n",
    "        The output depends on the **decode** method chosen.\n",
    "        \"\"\"\n",
    "        assert decode in self.decode, \"decode `{}` is not valid\".format(decode)\n",
    "        \n",
    "        if decode is 'posterior':\n",
    "            return self.posterior_decode(x)\n",
    "        \n",
    "        if decode is 'viterbi':\n",
    "            return self.viterbi_decode(x)\n",
    "\n",
    "    def compute_state_posteriors(self, x:list):\n",
    "        log_f_x, log_likelihood = self.log_forward_computations(x)\n",
    "        log_b_x, log_likelihood = self.log_backward_computations(x)\n",
    "        state_posteriors = np.zeros((self.n_states, len(x)))\n",
    "        \n",
    "        for pos in range(len(x)):\n",
    "            state_posteriors[:, pos] = log_f_x[:, pos] + log_b_x[:, pos] - log_likelihood\n",
    "        return state_posteriors\n",
    "\n",
    "    def posterior_decode(self, x: list, decode_states=True):\n",
    "        \n",
    "        state_posteriors = self.compute_state_posteriors(x)\n",
    "        y_hat = state_posteriors.argmax(axis=0)\n",
    "        \n",
    "        if decode_states:\n",
    "            y_hat = [hmm.pos_to_state[y] for y in y_hat]\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:52.938066Z",
     "start_time": "2022-04-05T05:09:52.936019Z"
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(word_to_pos, state_to_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:53.346641Z",
     "start_time": "2022-04-05T05:09:53.343403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rainy': 0, 'sunny': 1}, {'walk': 0, 'shop': 1, 'clean': 2, 'tennis': 3})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.state_to_pos, hmm.word_to_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:54.019077Z",
     "start_time": "2022-04-05T05:09:54.016848Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [t.x for t in train_sequences]\n",
    "Y = [t.y for t in train_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:54.324615Z",
     "start_time": "2022-04-05T05:09:54.321829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:85: RuntimeWarning: divide by zero encountered in log\n",
      "  return {\"emission\":   np.log(probs[\"emission\"]),\n",
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  \"transition\": np.log(probs[\"transition\"]),\n",
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "  \"final\":      np.log(probs[\"final\"]),\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(word_to_pos, state_to_pos)\n",
    "hmm.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:54.671771Z",
     "start_time": "2022-04-05T05:09:54.668309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission \n",
      " [[3. 1. 0. 0.]\n",
      " [2. 3. 3. 0.]] \n",
      "\n",
      "transition \n",
      " [[2. 0.]\n",
      " [2. 5.]] \n",
      "\n",
      "final \n",
      " [0. 3.] \n",
      "\n",
      "initial \n",
      " [2. 1.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in hmm.counts:\n",
    "    print(k,'\\n', hmm.counts[k],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:55.707205Z",
     "start_time": "2022-04-05T05:09:55.703759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission \n",
      " [[0.75  0.25  0.    0.   ]\n",
      " [0.25  0.375 0.375 0.   ]] \n",
      "\n",
      "transition \n",
      " [[0.5   0.   ]\n",
      " [0.5   0.625]] \n",
      "\n",
      "final \n",
      " [0.    0.375] \n",
      "\n",
      "initial \n",
      " [0.66666667 0.33333333] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in hmm.probs:\n",
    "    print(k,'\\n', hmm.probs[k],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:56.250646Z",
     "start_time": "2022-04-05T05:09:56.247076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission \n",
      " [[-0.28768207 -1.38629436        -inf        -inf]\n",
      " [-1.38629436 -0.98082925 -0.98082925        -inf]] \n",
      "\n",
      "transition \n",
      " [[-0.69314718        -inf]\n",
      " [-0.69314718 -0.47000363]] \n",
      "\n",
      "final \n",
      " [       -inf -0.98082925] \n",
      "\n",
      "initial \n",
      " [-0.40546511 -1.09861229] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in hmm.scores:\n",
    "    print(k,'\\n', hmm.scores[k],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Efficient forward probability computation\n",
    "\n",
    "The forward probability represents the probability that in position\n",
    "$i$ we are in state $Y_i = c_k$ and that we have observed $x_1,\\ldots,x_i$\n",
    "up to that position. Therefore, its mathematical expression is:\n",
    "\\begin{equation}\n",
    "\\mathbf{Forward \\ Probability\\!:}\\;\\;\\;\\;  \\mathrm{forward}(i, c_k) = P(Y_i = c_k, X_1=x_1,\\ldots, X_i = x_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\mathrm{forward}(i, c_k)$ using all the forward computations \\{$\\mathrm{forward}(i -1, c)$ for $c \\in \\Lambda$\\}. In order to facilitate the notation of the following argument we will denote by $x_{i:j}$  the assignemnt $X_i = x_i, \\dots, X_j = x_j$. Therefore we can write   $\\mathrm{forward}(i, y_i) $ as $P( y_i, x_{1:i } ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( y_i, x_{1:i } ) =  \\sum_{y_{i-1} \\in \\Lambda} P( y_i ,y_{i-1}, x_{1:i } )  =  \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) \\cdot P(y_i  | y_{i-1},  x_{1:i-1 }) \\cdot P(y_{i-1},  x_{1:i-1 })  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the **Observation independence** and the **Independence of previous states** properties of the first order HMM we have $P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) = P( x_i  | y_i) $ and $P(y_i  | y_{i-1},  x_{1:i-1 })  = P(y_i  | y_{i-1})  $. Therefore the previous equation can be written, \n",
    "for $i \\in \\{2,\\dots,N\\}$ (where $N$ is the length of the sequence), as \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathrm{forward}(i, y_i)  = \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i, ) \\cdot P(y_i  | y_{i-1}) \\cdot \\mathrm{forward}(i-1, y_{i-1})   \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The previous equation proves that  the forward probability can be defined by the\n",
    "following recurrence rule: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{forward}(1, c_k)&=& P_{\\text{init}}(c_k|\\text{start}) \\times P_{\\mathrm{emiss}}(x_1 | c_k)\n",
    " \\\\\n",
    " \\mathrm{forward}(i, c_k) &=& \\left(  \\sum_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{forward}(i-1, c_l) \\right) \\times P_{\\mathrm{emiss}}(x_i | c_k) \n",
    " \\\\\n",
    "  \\mathrm{forward}(N+1, \\text{stop}) &=& \\sum_{c_l \\in \\Lambda} P_{\\text{final}}(\\text{ stop} | c_l) \\times \\mathrm{forward}(N, c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Using the forward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{forward}(N+1, \\text{ stop}).\n",
    "\\end{equation}\n",
    "\n",
    "Although the forward probability is enough to calculate the likelihood of a given sequence, we will also need the backward probability to calculate the state posteriors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:57.187712Z",
     "start_time": "2022-04-05T05:09:57.184714Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'walk', 'shop', 'clean']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_sequences[1].x\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:57.687386Z",
     "start_time": "2022-04-05T05:09:57.685134Z"
    }
   },
   "outputs": [],
   "source": [
    "log_forward, loglikelihood = hmm.log_forward_computations(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:58.166306Z",
     "start_time": "2022-04-05T05:09:58.162668Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69314718, -1.67397643, -3.75341798,        -inf],\n",
       "       [-2.48490665, -2.58334672, -2.94017562, -4.08740307]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:58.517164Z",
     "start_time": "2022-04-05T05:09:58.513157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.068232326005127"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Efficient backward probability computation\n",
    "\n",
    "\n",
    "\n",
    "The backward probability is similar to the forward probability, but operates in the inverse direction.\n",
    "It represents the probability of observing $x_{i+1},\\ldots,x_N$ from position $i+1$ up to $N$, given that at position $i$ we are at state $Y_i = c_l$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Backward \\ Probability\\!:}\\;\\;\\;\\;  \\text{backward}(i, c_l) = P(X_{i+1}=x_{i+1},\\ldots, X_N=x_N | Y_i = c_l).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\text{backward}(i, c_k)$ using all the backward computations $\\text{backward}(i +1, c)$ for $c \\in \\Lambda$.\n",
    "\n",
    "Therefore we can write   $\\text{backward}(i, y_i) $ as $P( x_{i+1:N} | y_i ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( x_{i+1:N} | y_i ) =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+1:N}, y_{i+1} | y_i)  =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+2:N} | y_i, y_{i+1}, x_{i+1}) \n",
    "   P( x_{i+1}, |  y_{i+1},  y_{i}) P( y_{i+1} | y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Using the previous equation we have proved that the backward probability can be defined by the following recurrence rule:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{backward}(N, c_l) &=& P_{\\text{final}}(\\text{stop} | c_l)  \\\\\n",
    "\\text{backward}(i, c_l) &=&  \\displaystyle \\sum_{c_k \\in \\Lambda} P_{\\text{trans}}(c_k | c_l) \\times \n",
    "\\text{backward}(i+1, c_k) \\times P_{\\text{emiss}}(x_{i+1} | c_k) \n",
    " \\\\\n",
    "  \\mathrm{backward}(0, \\text{start}) &=& \\sum_{c_k \\in \\Lambda} P_{\\mathrm{init}}(c_k | \\text{ start}) \\times \\mathrm{backward}(1, c_k) \\times P_{\\mathrm{emiss}}(x_{1} | c_k).\n",
    " \\end{eqnarray}\n",
    "\n",
    "Using the backward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{backward}(0, \\text{start}).\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:59.194332Z",
     "start_time": "2022-04-05T05:09:59.191385Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'walk', 'shop', 'clean']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_sequences[1].x\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:59.823604Z",
     "start_time": "2022-04-05T05:09:59.821160Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_backward, loglikelihood_b = hmm.log_backward_computations(example)\n",
    "log_forward,  loglikelihood_f = hmm.log_forward_computations(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:00.550091Z",
     "start_time": "2022-04-05T05:10:00.546824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.068232326005126, -5.068232326005127)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood_b, loglikelihood_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:00.886591Z",
     "start_time": "2022-04-05T05:10:00.883335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.41863845, -3.67819455, -2.65480569,        -inf],\n",
       "       [-5.73879301, -3.88249502, -2.43166214, -0.98082925]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:01.208107Z",
     "start_time": "2022-04-05T05:10:01.205780Z"
    }
   },
   "outputs": [],
   "source": [
    "state_pos = hmm.compute_state_posteriors(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:01.505760Z",
     "start_time": "2022-04-05T05:10:01.502947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_pos.argmax(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a HMM in the conll data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:02.561930Z",
     "start_time": "2022-04-05T05:10:02.206445Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=100, max_nr_sent=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:02.802675Z",
     "start_time": "2022-04-05T05:10:02.797869Z"
    }
   },
   "outputs": [],
   "source": [
    "ind_to_word  = {v: k for k, v in train_seq.x_dict.items()}\n",
    "ind_to_state = {v: k for k, v in train_seq.y_dict.items()}\n",
    "word_to_ind  = train_seq.x_dict\n",
    "state_to_ind = train_seq.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:03.443297Z",
     "start_time": "2022-04-05T05:10:03.422482Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(len(train_seq)):\n",
    "    xy = train_seq[i]\n",
    "    X.append([ind_to_word[x_i] for x_i in xy.x])\n",
    "    Y.append([ind_to_state[y_i] for y_i in xy.y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:04.414661Z",
     "start_time": "2022-04-05T05:10:04.411214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'an',\n",
       " 'Oct.',\n",
       " '19',\n",
       " 'review',\n",
       " 'of',\n",
       " '``',\n",
       " 'The',\n",
       " 'Misanthrope',\n",
       " \"''\",\n",
       " 'at',\n",
       " 'Chicago',\n",
       " \"'s\",\n",
       " 'Goodman',\n",
       " 'Theatre',\n",
       " '(',\n",
       " '``',\n",
       " 'Revitalized',\n",
       " 'Classics',\n",
       " 'Take',\n",
       " 'the',\n",
       " 'Stage',\n",
       " 'in',\n",
       " 'Windy',\n",
       " 'City',\n",
       " ',',\n",
       " \"''\",\n",
       " 'Leisure',\n",
       " '&',\n",
       " 'Arts',\n",
       " ')',\n",
       " ',',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'Celimene',\n",
       " ',',\n",
       " 'played',\n",
       " 'by',\n",
       " 'Kim',\n",
       " 'Cattrall',\n",
       " ',',\n",
       " 'was',\n",
       " 'mistakenly',\n",
       " 'attributed',\n",
       " 'to',\n",
       " 'Christina',\n",
       " 'Haag',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:07.801048Z",
     "start_time": "2022-04-05T05:10:07.685671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:85: RuntimeWarning: divide by zero encountered in log\n",
      "  return {\"emission\":   np.log(probs[\"emission\"]),\n",
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  \"transition\": np.log(probs[\"transition\"]),\n",
      "/var/folders/05/h71x7gh54sx_5y43ppkq9_dw0000gq/T/ipykernel_3045/17578588.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "  \"final\":      np.log(probs[\"final\"]),\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(word_to_ind, state_to_ind)\n",
    "hmm.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:08.267195Z",
     "start_time": "2022-04-05T05:10:08.239775Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = hmm.predict_labels(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to make a prediction\n",
    "\n",
    "use `hmm.predict_labels ` to get the set of part of speech tags for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:09.546305Z",
     "start_time": "2022-04-05T05:10:09.519253Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = hmm.predict_labels(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:09.975649Z",
     "start_time": "2022-04-05T05:10:09.972739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre ( `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts ) , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:10.426196Z",
     "start_time": "2022-04-05T05:10:10.423169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adp det noun num noun adp . det noun . adp noun prt noun noun . . verb noun verb det noun adp noun noun . . noun conj noun . . det noun adp noun . verb adp noun noun . verb adv verb prt noun noun .'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:10.826998Z",
     "start_time": "2022-04-05T05:10:10.824655Z"
    }
   },
   "outputs": [],
   "source": [
    "result = skseq.sequences.sequence.Sequence(X[0],y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:11.211228Z",
     "start_time": "2022-04-05T05:10:11.208398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In/adp an/det Oct./noun 19/num review/noun of/adp ``/. The/det Misanthrope/noun ''/. at/adp Chicago/noun 's/prt Goodman/noun Theatre/noun (/. ``/. Revitalized/verb Classics/noun Take/verb the/det Stage/noun in/adp Windy/noun City/noun ,/. ''/. Leisure/noun &/conj Arts/noun )/. ,/. the/det role/noun of/adp Celimene/noun ,/. played/verb by/adp Kim/noun Cattrall/noun ,/. was/verb mistakenly/adv attributed/verb to/prt Christina/noun Haag/noun ./. "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure accuracy in the trian set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:12.120917Z",
     "start_time": "2022-04-05T05:10:12.106945Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.191152Z",
     "start_time": "2022-04-05T05:10:12.652854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:58<00:00, 85.45it/s]\n"
     ]
    }
   ],
   "source": [
    "Y_hat = []\n",
    "for x in tqdm.tqdm(X):\n",
    "    Y_hat.append(hmm.predict_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.323077Z",
     "start_time": "2022-04-05T05:11:11.294147Z"
    }
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "for y,y_hat in zip(Y,Y_hat):\n",
    "    for y_hat_k, y_k in zip(y,y_hat):\n",
    "        total +=1\n",
    "        if y_hat_k == y_k:\n",
    "            correct +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.433949Z",
     "start_time": "2022-04-05T05:11:11.429660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116935, 120065)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.550240Z",
     "start_time": "2022-04-05T05:11:11.546721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy posterior decode train data 0.9739307874901095\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy posterior decode train data\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure accuracy in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.679766Z",
     "start_time": "2022-04-05T05:11:11.675809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.805321Z",
     "start_time": "2022-04-05T05:11:11.796253Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(len(test_seq)):\n",
    "    xy = train_seq[i]\n",
    "    X_test.append([ind_to_word[x_i] for x_i in xy.x])\n",
    "    Y_test.append([ind_to_state[y_i] for y_i in xy.y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:23.635391Z",
     "start_time": "2022-04-05T05:11:11.917216Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:11<00:00, 85.37it/s]\n"
     ]
    }
   ],
   "source": [
    "Y_test_hat = []\n",
    "for x in tqdm.tqdm(X_test):\n",
    "    Y_test_hat.append(hmm.predict_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:23.644626Z",
     "start_time": "2022-04-05T05:11:23.636776Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_test = 0\n",
    "total_test   = 0\n",
    "for y,y_hat in zip(Y_test,Y_test_hat):\n",
    "    for y_hat_k, y_k in zip(y,y_hat):\n",
    "        total_test +=1\n",
    "        if y_hat_k == y_k:\n",
    "            correct_test +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:23.757553Z",
     "start_time": "2022-04-05T05:11:23.754362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy posterior decode test data 0.9734803459441734\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy posterior decode test data\", correct_test/total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
