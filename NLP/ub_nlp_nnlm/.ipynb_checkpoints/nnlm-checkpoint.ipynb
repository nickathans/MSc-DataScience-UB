{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yz9PnKJG4Vr"
   },
   "source": [
    "# Transformers for Machine Translation Tasks\n",
    "\n",
    "The purpose of this notebook is to work with transformer models for machine translation (MT) using three different approaches:\n",
    "1. Training a transformer model from scratch.\n",
    "2. Using a pre-trained model.\n",
    "3. Fine-tuning a pre-trained model.\n",
    "\n",
    "The translation task we are going to study constitutes an early example of a data driven task defined in the field of MT. The task is called EUTRANS-I or simply, the Traveller task, and its goal is to translate, from Spanish to English, a set of sentences involving human-to-human communication situations in the front-desk of a hotel. EUTRANS-I was generally tackled by means of statistical machine translation (SMT) techniques. SMT was the state-of-the-art technology preceeding the advent of NLP deep learning applications. EUTRANS-I is going to be useful for us due to its simplicity and small size, which will allow us to quickly execute translation experiments.\n",
    "\n",
    "An MT dataset typically consists of parallel files containing sentences or paragraphs in the source language and their corresponding translation in the target language. Separated pairs of files are often provided for training, validation and testing purposes. The EUTRANS-I original dataset was composed of 10K sentence pairs for training and 3K sentence pairs for testing, with no validation set. For this session, the original test part has been randomly shuffled and used to generate two subsets of 500 sentence pairs that will be used for validation and test sets (we are not using the whole original test set in order to speed up calculations).\n",
    "\n",
    "In addition to this, a smaller version of the EUTRANS-I dataset with only 2000 training pairs has also been created for this session. This dataset may be useful for debugging purposes or to speed up calculations if GPUs are not available (the notebook is configured to run on GPUs but their availability depend on different factors).\n",
    "\n",
    "Both versions of the EUTRANS-I dataset are included in the materials for this session and it is recommended that the corresponding folders are put in a Google Drive folder. This is because the easiest way to execute this notebook is to resort to Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6DhCElwcDwJ"
   },
   "source": [
    "## Translating with a Model Trained From Scratch\n",
    "\n",
    "The first approach to MT we are going to explore consists in defining a transformer model and training it from scratch. Working with deep learning architectures has become increasingly affordable thanks to two libraries: [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/). This session is going to use PyTorch. PyTorch libraries can be installed locally or in the cloud (if we use Google Colab). Some of the commands executed in this notebook are used to install the required software. In spite of the fact that some requested package versions are no longer the latest ones, they have been chosen here so as to avoid current bugs or incompatibilities.\n",
    "\n",
    "The content of this section is based on the [official documentation of Pytorch for transformer models](https://pytorch.org/tutorials/beginner/translation_transformer.html). The most relevant modifications that haven been made here are related to data handling and also to enable the comparison of the translation quality results of this approach with the other two approaches to be tested. In addition to this, the code has largely been reordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlWzlltmHQYR"
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jj2UKPLeG4Vy"
   },
   "source": [
    "### Define the Transformer Code\n",
    "\n",
    "The definition of the transformer is based on the foundational paper [Attention is All you Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). The defined network has three parts:\n",
    "\n",
    "1. The embedding layer, which maps a sparse representation of source tokens into a dense one. The embeddings are further augmented with positional encodings to provide position information.\n",
    "2. The actual transformer model.\n",
    "3. A linear output layer providing un-normalized probabilities for each target language token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TycMwN9PG4Vz"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1gEYJrpG4V5"
   },
   "source": [
    "### Define Data Related Functions\n",
    "\n",
    "The following code defines different functions related to data handling, including:\n",
    "\n",
    "- A class to create a Pytorch dataset from parallel text files.\n",
    "- Functions related to word masking  that will prevent the model from looking into the future words when making predictions.\n",
    "- Functions to convert data samples into numerical vectors or *tensors* that will be manipulated inside the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9XnRBTcG4V6"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "class PlainTextParallelFilesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_fname, trg_fname) -> None:\n",
    "        self.src_fname = src_fname\n",
    "        self.trg_fname = trg_fname\n",
    "        self.source = open(self.src_fname, encoding='utf-8').read().split('\\n')\n",
    "        self.target = open(self.trg_fname, encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        # load one sample by index, e.g like this:\n",
    "        source_sample = self.source[idx]\n",
    "        target_sample = self.target[idx]\n",
    "\n",
    "        return source_sample, target_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def get_source(self):\n",
    "        return self.source\n",
    "\n",
    "    def get_target(self):\n",
    "        return self.target\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8UWPm7yG4V7"
   },
   "source": [
    "### Define Training, Loss Evaluation and Decoding Functions\n",
    "\n",
    "The following code defines functions useful to train and evaluate the model, and also to infer the translation of sentences that were not seen during the training stage (the translation process is also referred to as *decoding*).\n",
    "\n",
    "Training a deep neural network is a complex and sophisticated process that cannot be explained here in a detailed manner. An intuitive idea behind model training is that the internal weights (or parameters) of the network are adjusted so as to allow the model to generate the target translation contained in the training set for each sentence in the source language. When the model translates each source sentence, the generated translation may not be exactly equal to the one given as reference. The amount of difference between the generated outputs and the references is measured by means of a mathematical function called *loss function*. The resulting loss is used to adjust the weights of the network so that it can make better predictions in the future. Typically, we are interested in calculating the loss of the model, not only for the training set, but also for the validation set, so as to detect common machine learning problems such as model overfitting.\n",
    "\n",
    "The training process is structured as a series of *epochs*. During training, we say that an epoch was executed when every example in the training dataset has been used once to update the model parameters.\n",
    "\n",
    "Before starting the training process, the input text is preprocessed. The goal of preprocessing is to make the modelling process easier and more effective. One fundamental step of text preprocessing is *tokenization*. Tokenization, in its most basic definition consists in fragmenting the raw text to be translated into individual words or tokens, although it may involve additional and more complex operations. Typically, the tokenization process will separate punctuation symbols from words. For instance, the string `\"Hello World!\"` would be tokenized into the word vector `[\"Hello\", \"World\", \"!\"]`.\n",
    "\n",
    "Since the model works with tokenized data, its output is also going to be tokenized unless we apply a *detokenization* step. This step has been incorporated into the code of the `translate` function so as to enable the comparison of translation quality results between approaches. This is because the standard translation quality metric to be applied expects raw text as input (see more on this below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkmyF0HWG4V7"
   },
   "outputs": [],
   "source": [
    "!pip install mosestokenizer sacremoses\n",
    "\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer, train_iter, loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "def evaluate_loss(model, val_iter, loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    word_list = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
    "    for sym in [\"<bos>\", \"<eos>\"]:\n",
    "      if sym in word_list:\n",
    "        word_list.remove(sym)\n",
    "    return MosesDetokenizer('TGT_LANGUAGE')(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7QAUskAKs-i"
   },
   "source": [
    "### Enable Metrics Computation\n",
    "\n",
    "A fundamental aspect of building a machine learning system is evaluating its performance. The evaluation can be manual or automatic. Typically, manual evaluation is expensive and subjective, since it requires the participation of human experts. On the other hand, automatic evaluation measures are cheap and objective, even if they are not able to capture information that a human expert evaluation would provide.\n",
    "\n",
    "One of the standard translation quality measures defined in the field of MT is the [BLEU score](https://aclanthology.org/P02-1040.pdf). Given a reference text and the text generated by a translation system, the BLEU score is a number between 0 and 1 which measures the translation quality, being 1 a perfect match. The BLEU score is sometimes expressed as a percentage.\n",
    "\n",
    "The BLEU score can be easily calculated by installing the `evaluate` package provided by the [Hugging Face library](https://huggingface.co/), which will also be used to work with pre-trained models. The `evaluate` package contains an advanced implementation of the BLEU score called `sacrebleu`. One important aspect of BLEU score calculation with `sacrebleu` is that the text should be provided in raw form, resulting in the necessity of incorporating a detokenization step for the translation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnwyP0PpKxAS"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate sacrebleu\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19xrM6aFG4V8"
   },
   "source": [
    "### Prepare Data\n",
    "\n",
    "Before starting the training process, we need to load and preprocess data. For this purpose, a parallel dataset object is created for the training, validation and test sets that compose the EUTRANS-I translation task. In addition to this, each sentence to be processed by the model should be tokenized and the resulting words converted into numerical identifiers. Finally, special symbols (such as the begin-of-sentence or end-of-sentence symbols) are added to the vector of identifiers before transforming it into a tensor that can be manipulated inside the network.\n",
    "\n",
    "On the other hand, it is important to stress out that the parallel files of the EUTRANS-I translation task are assumed to be stored in a Google Drive directory whose path should be provided in the code below. If the notebook is executed locally instead of using Google Colab, then the files should be handled in a different way and the code would require modifications accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldGyEa81G4V9"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set variables\n",
    "SRC_LANGUAGE = 'es'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# https://pytorch.org/text/stable/data_utils.html\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer(\"moses\")\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer(\"moses\")\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# Data iterators\n",
    "# NOTE: EDIT THE CONTENT OF THE \"datadir\" VARIABLE TO POINT TO THE GOOGLE DRIVE\n",
    "#       DIRECTORY WHERE YOU HAVE COPIED YOUR FILES. IT CAN BE INTERESTING TO \n",
    "#       START USING THE SMALL VERSION OF THE DATASET AND AFTER FINISHING THE\n",
    "#       EXPERIMENTS REPEAT THEM WITH THE LARGER VERSION\n",
    "#datadir = \"/content/drive/MyDrive/Colab Notebooks/ub_nlp_nnlm/data/\"\n",
    "datadir = \"/content/drive/MyDrive/Colab Notebooks/ub_nlp_nnlm/data_small/\"\n",
    "train_iter = PlainTextParallelFilesDataset(datadir + \"es.train\", datadir + \"en.train\")\n",
    "val_iter = PlainTextParallelFilesDataset(datadir + \"es.valid\", datadir + \"en.valid\")\n",
    "test_iter = PlainTextParallelFilesDataset(datadir + \"es.test\", datadir + \"en.test\")\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "    vocab_transform[ln].set_default_index(vocab_transform[ln]['<unk>'])\n",
    "    \n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "# NOTE: sequential_transforms is a composition of functions that is stored in \n",
    "#       text_transform. text_transform is later used in collate function\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenization\n",
    "                                               vocab_transform[ln], # Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKY67kU_G4V-"
   },
   "source": [
    "### Instantiate Model\n",
    "\n",
    "The code below instantiates a `Seq2SeqTransformer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yJ81KyEG4V_"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MckGbBuAG4V_"
   },
   "source": [
    "### Train Model\n",
    "\n",
    "At this point, everything is ready to start training the model by completing a series of epochs. After each epoch is completed, the loss of the training and validation sets is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4XAGXqaG4WA"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 5\n",
    "  \n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, train_iter, loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate_loss(transformer, val_iter, loss_fn)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acB_L9VbG4WA"
   },
   "source": [
    "### Generate Sample Translations\n",
    "\n",
    "After training the model, we are ready to use it for generating some translations. To do this we only need to call the `translate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grFoeHWbG4WA"
   },
   "outputs": [],
   "source": [
    "print(translate(transformer, \"me gustaría cambiarme a otra habitación con teléfono, por favor.\"))\n",
    "print(translate(transformer, \"Me agradaría cambiarme a otra habitación con teléfono, por favor.\"))\n",
    "print(translate(transformer, \"Esto es un ejemplo de frase que no pertenece al contexto del lenguaje usado en el mostrador de un hotel.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLXF8Bxup7zq"
   },
   "source": [
    "### Compute Metrics for Test Set (**Exercise**)\n",
    "\n",
    "Finally, we calculate the BLEU score when translating the EUTRANS-I test set with the trained transformer model. Completing this code is left as an exercise. Consult the documentation about BLEU score computation [here](https://huggingface.co/spaces/evaluate-metric/bleu) if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC3YMW3fp8Tl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Obtain translations for test set\n",
    "# TO-BE-DONE\n",
    "\n",
    "# Show 5 random source/prediction pairs\n",
    "# TO-BE-DONE\n",
    "\n",
    "# Compute BLEU score\n",
    "# TO-BE-DONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60ebpxbbG4WB"
   },
   "source": [
    "## Translating with a Pre-Trained Model\n",
    "\n",
    "Instead of training a translation system from scratch, it is becoming more and more frequent to use the so-called *pre-trained* models. Pre-trained models are general machine learning models composed of a huge number of parameters which have been estimated on massive amounts of training data. This pre-training process is very costly in terms of computational resources and thus not affordable to regular machine learning practitioners. However, a pre-trained model can be downloaded by a regular user and utilized as a starting point to implement a machine learning system for a particular task using much less data and computational resources. This goal can be achieved by means of *transfer learning* techniques, and in particular *fine-tuning*. The term fine-tuning refers to the process of adapting the parameters of a pre-trained model to a particular task using a small, task-specific dataset.\n",
    "\n",
    "Despite the fact that pre-trained systems often are not intended to be applied directly, in this section we are going to use and evaluate a pre-trained model without fine-tuning for educational purposes (the final fine-tuning stage will be executed and evaluated separately at the end of this notebook).\n",
    "\n",
    "In a similar way to what happened with Pytorch or Tensorflow for deep learning, working with pre-trained models is now far easier by means of the tools provided by [Hugging Face](https://huggingface.co/), a company that develops open-source software libraries for natural language processing and other machine learning tasks, with a strong emphasis on providing pre-trained transformers.\n",
    "\n",
    "We start the work in this section by installing some Hugging Face packages useful to work with pre-trained models. \n",
    "\n",
    "The rest of the content is again an adapted version of official documentation, in this case provided by Hugging Face [here](https://huggingface.co/docs/transformers/tasks/translation). This documentation can be particularly useful to find more information about the different classes and functions that are going to be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf1czle5G4WB"
   },
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "!pip install transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C97_kM3iclQd"
   },
   "source": [
    "### Generate Datasets\n",
    "\n",
    "The first thing we will do is to load the EUTRANS-I files into Huggin Face `DataSet` objects. It is worthy of note that in addition to the training, validation and test parallel files, we now have an additional pair of files for fine-tuning (files with extension `finetun`). These files just contain a small subset of a few hundred sentences extracted from the training set (remember that the training set contains 10K sentence pairs). We will use this smaller training dataset to simulate a hypotetical scenario where we have limited training data to fine-tune the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlfSGKsccR36"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def gen_dict_from_parallel_files(srclang, trglang, src_fname, trg_fname):\n",
    "  # Load files into lists\n",
    "  srclist = open(src_fname, encoding='utf-8').read().split('\\n')\n",
    "  trglist = open(trg_fname, encoding='utf-8').read().split('\\n')\n",
    "  # Create dictionary\n",
    "  dpairs = {}\n",
    "  dpairs['id'] = []\n",
    "  dpairs['translation'] = []\n",
    "  id = 0\n",
    "  for src, trg in zip(srclist, trglist):\n",
    "    if src and trg:\n",
    "      dpairs['id'].append(id)\n",
    "      pair = {}\n",
    "      pair[srclang] = src\n",
    "      pair[trglang] = trg\n",
    "      dpairs['translation'].append(pair)\n",
    "      id += 1\n",
    "  # Return dictionary\n",
    "  return dpairs\n",
    "\n",
    "def gen_dsetdict_from_parallel_files(srclang, trglang, srcpref, trgpref): \n",
    "  # Generate dictionaries \n",
    "  train_dict = gen_dict_from_parallel_files(srclang, trglang, srcpref + \".train\", trgpref + \".train\")\n",
    "  finetun_dict = gen_dict_from_parallel_files(srclang, trglang, srcpref + \".finetun\", trgpref + \".finetun\")\n",
    "  valid_dict = gen_dict_from_parallel_files(srclang, trglang, srcpref + \".valid\", trgpref + \".valid\")\n",
    "  test_dict = gen_dict_from_parallel_files(srclang, trglang, srcpref + \".test\", trgpref + \".test\")\n",
    "  # Generate Dataset Dictionary\n",
    "  dset = DatasetDict()\n",
    "  dset[\"train\"] = Dataset.from_dict(train_dict)\n",
    "  dset[\"finetun\"] = Dataset.from_dict(finetun_dict)\n",
    "  dset[\"valid\"] = Dataset.from_dict(valid_dict)\n",
    "  dset[\"test\"] = Dataset.from_dict(test_dict)\n",
    "  return dset\n",
    "\n",
    "# Generate datasets\n",
    "SRC_LANGUAGE = 'es'\n",
    "TGT_LANGUAGE = 'en'\n",
    "# NOTE: EDIT THE CONTENT OF THE \"datadir\" VARIABLE TO POINT TO THE GOOGLE DRIVE\n",
    "#       DIRECTORY WHERE YOU HAVE COPIED YOUR FILES. IT CAN BE INTERESTING TO \n",
    "#       START USING THE SMALL VERSION OF THE DATASET AND AFTER FINISHING THE\n",
    "#       EXPERIMENTS REPEAT THEM WITH THE LARGER VERSION\n",
    "#datadir = \"/content/drive/MyDrive/Colab Notebooks/ub_nlp_nnlm/data/\"\n",
    "datadir = \"/content/drive/MyDrive/Colab Notebooks/ub_nlp_nnlm/data_small/\"\n",
    "dset = gen_dsetdict_from_parallel_files(SRC_LANGUAGE, TGT_LANGUAGE, datadir + \"es\", datadir + \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I4bNxSmqweK"
   },
   "source": [
    "### Preprocess Datasets\n",
    "\n",
    "After creating the `DataSet` objects, we will tokenize the data. In Hugging Face, each pre-trained model may also incorporate a specific tokenization process. \n",
    "\n",
    "Since the tokenizer is linked to its pre-trained model, the first thing we should do is to decide which model we want to use. A particular model in Hugging Face can be identified by its *checkpoint*. More specifically, a checkpoint refers to a saved state of a trained deep learning model that contains the values of its weights and other important parameters. Checkpoints are created during the training process at certain intervals or after achieving a specific performance metric. These saved checkpoints can be used to resume training from where it was left off or to make predictions with the trained model, which is what we are about to do.\n",
    "\n",
    "When using the Hugging Face library, a checkpoint is represented by means of a string value. For this session, we have chosen a transformer model with checkpoint [Helsinki-NLP/opus-mt-es-en](https://huggingface.co/Helsinki-NLP/opus-mt-en-es). The tokenizer for this model can be instantiated by means of an `Autotokenizer` object for which we call the `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOTeGIe6q5kW"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def extract_text(examples, lang):\n",
    "    return [example[lang] for example in examples[\"translation\"]]\n",
    "\n",
    "def extract_inputs_targets(examples, srclang, trglang):\n",
    "    inputs = extract_text(examples, srclang)\n",
    "    targets = extract_text(examples, trglang)\n",
    "    return inputs, targets\n",
    "\n",
    "def preprocess_function(examples, srclang, trglang):\n",
    "    inputs, targets = extract_inputs_targets(examples, srclang, trglang)\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "checkpoint = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_dset = dset.map(preprocess_function, batched=True, fn_kwargs={\"srclang\": SRC_LANGUAGE, \"trglang\": TGT_LANGUAGE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQF4kVIZdLG-"
   },
   "source": [
    "### Generate Sample Translations\n",
    "\n",
    "At this point, we can start generating translations. For this purpose, we can use `pipeline` objects provided by the Hugging Face library. In Hugging Face, a `pipeline` object is a high-level interface for performing specific natural language processing tasks using pre-trained models.\n",
    "\n",
    "When creating a `pipeline` object, it is necessary to specify the task to be performed, such as text classification, named entity recognition, or question answering. The `pipeline` object then automatically loads the appropriate pre-trained model and tokenizer, and applies them to the input text to perform the specified task. Optionally, we can specify a checkpoint to instantiate a `pipeline` object, which is what is done in the code below.\n",
    "\n",
    "Finally, in order to use the newly created `pipeline` object, we only need to call it with the input text as its argument. The pipeline object will then process such input text and return the output of the NLP task. The input text can be an individual string or a list of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NifKXwjOdOKX"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=checkpoint)\n",
    "print(translator(\"me gustaría cambiarme a otra habitación con teléfono, por favor.\"))\n",
    "print(translator(\"Esto es un ejemplo de frase que no pertenece al contexto del lenguaje usado en el mostrador de un hotel.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DIbk_T9kLzx"
   },
   "source": [
    "### Enable Metrics Computation\n",
    "\n",
    "We enable computation of the BLEU score here again, so as to allow independent execution of the code related to pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SlDky91kNCI"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate sacrebleu\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRPoHlswY6Jb"
   },
   "source": [
    "### Compute Metrics for Test Set (**Exercise**)\n",
    "\n",
    "To end this part of the session, we will compute translation quality metrics for the pre-trained system. For this purpose, we will make use again of a `pipeline` object, but now we are going to process the sentences contained in the test set. Again, completing this code is left as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbzovAyQY9Q0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Obtain translations for test set\n",
    "# TO-BE-DONE\n",
    "\n",
    "# Show 5 random source/prediction pairs\n",
    "# TO-BE-DONE\n",
    "\n",
    "# Compute BLEU score\n",
    "# TO-BE-DONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B15UsyUXG4WC"
   },
   "source": [
    "## Translating with a Fine-Tuned Pre-Trained Model\n",
    "\n",
    "In the previous sections we have worked with a pre-trained model. In spite of the fact that the model generates correct translations, it is not prepared to generate text with the particular features of the text contained in the EUTRANS-I dataset.\n",
    "\n",
    "To solve this problem, we can use a small training set to fine-tune the model parameters of the pre-trained system. This fine-tuning strategy is very useful since it reduces the amount of training samples and computational power that are required to implement a translation system specific for a particular task, in contrast to the requirements of the approach adopted at the beginning of this notebook, based on training a transformer from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0PCZja8cxK1"
   },
   "source": [
    "### Instantiate Model\n",
    "\n",
    "Before fine-tuning the model, we need to create a model instance. For this purpose we use a particular class derived from the `AutoModel` class (`AutoModelForSeq2SeqLM`). In Hugging Face, `AutoModel` is a class that enables users to automatically load any pre-trained model, without the need to specify the particular model name or architecture. Instead, what we need to specify is the checkpoint identifying the model of interest.\n",
    "\n",
    "Once the `AutoModel` object is instantiated, executing its `from_pretrained` method will automatically download the weights for the model, as well as the corresponding tokenizer and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grz4D_HTc0Dt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "checkpoint = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "# NOTE: By default, when calling the previous method, the model will use GPUs if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e9Tfe6wouKx"
   },
   "source": [
    "### Fine-Tune Model\n",
    "\n",
    "The code below fine-tunes the pre-trained model we have used previously. The following three steps are executed:\n",
    "\n",
    "1. Define the training hyperparameters.\n",
    "2. Pass the training arguments (including the hyperparameters) to a `trainer` object. \n",
    "3. Call `train` method to fine-tune the model.\n",
    "\n",
    "The training process will be carried out over a partition specific for fine-tuning (files with `finetun` extension). After each epoch is executed, the loss for the validation set is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAOFNC4jG4WC"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_fine_tuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps = 1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dset[\"finetun\"],\n",
    "    eval_dataset=tokenized_dset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFjAD54ho-Zz"
   },
   "source": [
    "### Generate Sample Translations\n",
    "\n",
    "Now we can generate some example translations by means of a `pipeline` object defined for the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL_94BH_pBBr"
   },
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=trainer.model.to('cpu'), tokenizer=trainer.tokenizer, device=-1) # To simplify execution, we ensure that the computations are carried out on CPU\n",
    "print(translator(\"me gustaría cambiarme a otra habitación con teléfono, por favor.\"))\n",
    "print(translator(\"Esto es un ejemplo de frase que no pertenece al contexto del lenguaje usado en el mostrador de un hotel.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vS6YUupqFIp"
   },
   "source": [
    "### Compute Metrics for Test Set (**Exercise**)\n",
    "\n",
    "The last step to be executed is computing the BLEU score for the translations generated by the fine-tuned model for the EUTRANS-I test set. The step is left again as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KX_szZPMqFUB"
   },
   "outputs": [],
   "source": [
    "# Obtain translations for test set\n",
    "# TO-BE-DONE\n",
    "\n",
    "# Show 5 random source/prediction pairs\n",
    "# TO-BE-DONE\n",
    "\n",
    "# Compute BLEU score\n",
    "# TO-BE-DONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbwIbsKS9LSj"
   },
   "source": [
    "## Optional Exercises\n",
    "\n",
    "To complement the results that have been obtained previously, it can be interesting to try the following:\n",
    "\n",
    "- Play with different number of training iterations and model hyperparameters.\n",
    "- Extract larger portions of the training set and use them as the fine-tune set in order to measure the impact in the test translation quality.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
