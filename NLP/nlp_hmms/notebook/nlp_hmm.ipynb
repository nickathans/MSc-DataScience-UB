{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Basic algorithm for sequence models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Hidden markov models learning\n",
    "\n",
    "- Computing counts from a corpus\n",
    "- From counts to probabilities: finding the parameters of initial, transition and emission probabilities.\n",
    "- Smoothing non seen events in avoid overfitting.\n",
    "\n",
    "##  Working with scores instead of probabilities\n",
    "\n",
    "- Rewritting probabilities with log probabilities. \n",
    "- Why do we want to work with log probabilities? Understand issues when computing a product of probabilities.\n",
    "- The `logsumexp` trick.\n",
    "\n",
    "##  The forward backward algorithm\n",
    "\n",
    "- Computing the probability of a sequence $P(X=x)$\n",
    "- Computing the probability of a stage given an input sequence $P(Y_i=x \\,\\vert\\, X=x)$. \n",
    "- Use this probabilities to do Posterior decoding. Evaluate posterior decoding in a problem.\n",
    "\n",
    "\n",
    "## Computing the most likely hidden state sequence: The viterbi algorithm\n",
    "\n",
    "\n",
    "- How to compute $argmax_{y_1,\\dots,y_N \\in \\Lambda^N} P(Y_{1:N}=y_{1:N}\\,\\vert\\, X=x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:52.267473Z",
     "start_time": "2022-04-05T12:09:52.257673Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import os,sys,inspect\n",
    "#currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "#parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,'../') \n",
    "import skseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classes used to store the sequences\n",
    "\n",
    "We will use\n",
    "\n",
    "- class ``Sequence`` in ``skseq/sequences/sequence.py`` file\n",
    "- class ``LabelDictionary`` in ```skseq/sequences/label_dictionary.py`` file\n",
    "- class ``SequenceList`` in ```skseq/sequences/sequence_list.py`` file\n",
    "- class ``_SequenceIterator`` in ```skseq/sequences/sequence_list.py`` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a Sequence Object\n",
    "\n",
    "- Sequence objects are defined in ```skseq/sequences/sequence.py``. \n",
    "    - A sequence in a supervised learning problem consist on a set of words and tags associated to words.\n",
    "    - For example ``w_1/t_1 w_2/t_2 w_3/t_3`` is a sequence of lenght 3 with words ``w_i`` and tags ``t_i``.\n",
    "\n",
    "\n",
    "In order to instanciate a Sequence we essentially need a list of words and a list of tags of the same size. In order to do it efficiently we will not store strings for the words and tags. We will store an integer values that will represent words and tags.\n",
    "\n",
    "- **.x** attribute: list of words (integer words)\n",
    "\n",
    "- **.y** attribute: list of tags (integer tags)\n",
    "\n",
    "Then we need to keep a mapping from integers to words and from integers to tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:53.139830Z",
     "start_time": "2022-04-05T12:09:53.126133Z"
    }
   },
   "outputs": [],
   "source": [
    "import skseq\n",
    "import skseq.sequences\n",
    "import skseq.readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:53.526872Z",
     "start_time": "2022-04-05T12:09:53.518758Z"
    }
   },
   "outputs": [],
   "source": [
    "from skseq.sequences import sequence\n",
    "\n",
    "seq = skseq.sequences.sequence.Sequence(x=[1,3,2,4],\n",
    "                                            y=[0,2,1,1])\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:53.900060Z",
     "start_time": "2022-04-05T12:09:53.896561Z"
    }
   },
   "outputs": [],
   "source": [
    "seq = skseq.sequences.sequence.Sequence(x=[\"David\",\"was\",\"happy\"],\n",
    "                                            y=[\"NounEntity\",\"0\",\"0\"])\n",
    "\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a vocabulary and a SequenceList\n",
    "\n",
    "Given a training set with words and tags we want to build a SequenceList object definded in  ``skseq/sequences/sequence_list.py``.\n",
    "\n",
    "A  SequenceList is a class that is initialized using a\n",
    "- dictionary for the words\n",
    "- a dictionary for the tags\n",
    "- an empty sequence list where the Sequences read from the data will be stored.\n",
    "\n",
    "\n",
    "    class SequenceList(object):\n",
    "\n",
    "        def __init__(self, x_dict, y_dict):\n",
    "            self.x_dict = x_dict\n",
    "            self.y_dict = y_dict\n",
    "            self.seq_list = []\n",
    "\n",
    "\n",
    "Let us create 3 sequence list for train, test and validation.  \n",
    "\n",
    "We will use the conll dataset and the class  ``PostagCorpus``.\n",
    "The class has a method ``.read_sequence_list_conll`` that will return the **SequenceList** object we want\n",
    "\n",
    "\n",
    "\n",
    "    def read_sequence_list_conll(self, train_file,\n",
    "                                 mapping_file=(\"%s/en-ptb.map\"\n",
    "                                               % dirname(__file__)),\n",
    "                                 max_sent_len=100000,\n",
    "                                 max_nr_sent=100000):\n",
    "\n",
    "        # Build mapping of postags:\n",
    "        mapping = {}\n",
    "        if mapping_file is not None:\n",
    "            for line in open(mapping_file):\n",
    "                coarse, fine = line.strip().split(\"\\t\")\n",
    "                mapping[coarse.lower()] = fine.lower()\n",
    "\n",
    "        instance_list = self.read_conll_instances(train_file,\n",
    "                                                  max_sent_len,\n",
    "                                                  max_nr_sent,\n",
    "                                                  mapping)\n",
    "\n",
    "        seq_list = SequenceList(self.word_dict, self.tag_dict)\n",
    "\n",
    "        for sent_x, sent_y in instance_list:\n",
    "            seq_list.add_sequence(sent_x, sent_y,  self.word_dict, self.tag_dict)\n",
    "\n",
    "        return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:54.483769Z",
     "start_time": "2022-04-05T12:09:54.364326Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls ../skseq/data/CoNLL_2002_shared_task/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:54.825623Z",
     "start_time": "2022-04-05T12:09:54.708998Z"
    }
   },
   "outputs": [],
   "source": [
    "!head -20 ../skseq/data/CoNLL_2002_shared_task/esp.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:09:55.963113Z",
     "start_time": "2022-04-05T12:09:55.109729Z"
    }
   },
   "outputs": [],
   "source": [
    "import skseq.readers.pos_corpus\n",
    "corpus = skseq.readers.pos_corpus.PostagCorpus()\n",
    "\n",
    "data_path = \"../skseq/data/CoNLL_2002_shared_task\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll2002(data_path + \"/esp.train\", \n",
    "                                            max_sent_len=100, max_nr_sent=5000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll2002(data_path + \"/esp.testb\",\n",
    "                                           max_sent_len=100, max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll2002(data_path + \"/esp.testa\", \n",
    "                                          max_sent_len=100, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:12.765914Z",
     "start_time": "2022-04-05T12:10:12.762913Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(train_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:13.311804Z",
     "start_time": "2022-04-05T12:10:13.308996Z"
    }
   },
   "outputs": [],
   "source": [
    "type(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:13.868046Z",
     "start_time": "2022-04-05T12:10:13.864792Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:14.350647Z",
     "start_time": "2022-04-05T12:10:14.347886Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_seq.x_dict), len(train_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:14.830115Z",
     "start_time": "2022-04-05T12:10:14.827185Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:15.246565Z",
     "start_time": "2022-04-05T12:10:15.243607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(test_seq.x_dict), len(test_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:15.738014Z",
     "start_time": "2022-04-05T12:10:15.735148Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:16.131363Z",
     "start_time": "2022-04-05T12:10:16.128542Z"
    }
   },
   "outputs": [],
   "source": [
    "# first sentence\n",
    "train_seq[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:16.838216Z",
     "start_time": "2022-04-05T12:10:16.835199Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:17.580734Z",
     "start_time": "2022-04-05T12:10:17.577894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:19.076798Z",
     "start_time": "2022-04-05T12:10:19.073814Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set of possible tags Lambda\n",
    "# train_seq.y_dict is a dictionary of mappings from tag to integer id \n",
    "train_seq.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:19.785513Z",
     "start_time": "2022-04-05T12:10:19.782758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of possible words\n",
    "len(train_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:20.274527Z",
     "start_time": "2022-04-05T12:10:20.271531Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our corpus ``sequencelist`` to map integers to words\n",
    "\n",
    "Sequences can use ``SequenceList`` objects to map word_ids and tag_ids to words and tags.\n",
    "\n",
    "All ``sequence`` objects have the **``.to_words``** method which allows us to print the words given a **``SequenceList``** object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:21.405709Z",
     "start_time": "2022-04-05T12:10:21.402643Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:29.573888Z",
     "start_time": "2022-04-05T12:10:29.571436Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence = train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:30.109916Z",
     "start_time": "2022-04-05T12:10:30.107027Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T12:10:30.648194Z",
     "start_time": "2022-04-05T12:10:30.645179Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence.to_words(sequence_list=train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Hidden markov models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:27.950128Z",
     "start_time": "2022-04-05T05:09:27.948063Z"
    }
   },
   "outputs": [],
   "source": [
    "import skseq.sequences.sequence as seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:28.693545Z",
     "start_time": "2022-04-05T05:09:28.691300Z"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = [\"walk\", \"shop\", \"clean\", \"tennis\"]\n",
    "Lambda = [\"rainy\", \"sunny\"]\n",
    "\n",
    "sequence_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:29.019665Z",
     "start_time": "2022-04-05T05:09:29.015886Z"
    }
   },
   "outputs": [],
   "source": [
    "# walk/rainy walk/sunny shop/sunny clean/sunny \n",
    "# walk/rainy walk/rainy shop/rainy clean/sunny \n",
    "# walk/sunny shop/sunny shop/sunny clean/sunny \n",
    "\n",
    "s1 = seq.Sequence([\"walk\", \"walk\", \"shop\", \"clean\"],\n",
    "                  [\"rainy\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "s2 = seq.Sequence([\"walk\", \"walk\", \"shop\", \"clean\"], \n",
    "                  [\"rainy\", \"rainy\", \"rainy\", \"sunny\"])\n",
    "\n",
    "s3 = seq.Sequence([\"walk\", \"shop\", \"shop\", \"clean\"], \n",
    "                  [\"sunny\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "train_sequences = [s1, s2, s3];\n",
    "\n",
    "s1_t = seq.Sequence([\"walk\", \"walk\", \"shop\", \"clean\"], \n",
    "                    [\"rainy\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "s2_t = seq.Sequence([\"clean\", \"walk\", \"tennis\", \"walk\"], \n",
    "                    [\"sunny\", \"sunny\", \"sunny\", \"sunny\"])\n",
    "\n",
    "test_sequences = [s1_t, s2_t];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:29.332353Z",
     "start_time": "2022-04-05T05:09:29.329239Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:30.121087Z",
     "start_time": "2022-04-05T05:09:30.119135Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_pos  = {\"walk\":0,  \"shop\":1, \"clean\":2, \"tennis\":3}\n",
    "state_to_pos = {\"rainy\":0, \"sunny\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:30.837622Z",
     "start_time": "2022-04-05T05:09:30.833665Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_initial_counts(initial_counts, seq, state_to_pos):\n",
    "    pass\n",
    "\n",
    "def update_transition_counts(transition_counts, seq, state_to_pos):\n",
    "    pass\n",
    "\n",
    "        \n",
    "def update_emission_counts(emission_counts, seq, state_to_pos, word_to_pos):\n",
    "    pass\n",
    "\n",
    "def update_final_counts(final_counts, seq, state_to_pos):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the previous function we can train the emisssion, initial and transition probabilities by simply counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:31.776510Z",
     "start_time": "2022-04-05T05:09:31.773001Z"
    }
   },
   "outputs": [],
   "source": [
    "def sufficient_statistics_hmm(sequences, state_to_pos, word_to_pos):\n",
    "    \n",
    "    n_states = len(state_to_pos)\n",
    "    n_words  = len(word_to_pos)\n",
    "    initial_counts      = np.zeros((n_states))\n",
    "    transition_counts   = np.zeros((n_states, n_states))\n",
    "    final_counts        = np.zeros((n_states))\n",
    "    emission_counts     = np.zeros((n_states, n_words))\n",
    "    \n",
    "    for seq in sequences:\n",
    "        pass\n",
    "\n",
    "    return initial_counts, transition_counts, final_counts, emission_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:32.117708Z",
     "start_time": "2022-04-05T05:09:32.115580Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = sufficient_statistics_hmm(train_sequences, \n",
    "                                   state_to_pos,\n",
    "                                   word_to_pos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:32.501630Z",
     "start_time": "2022-04-05T05:09:32.499512Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_counts, transition_counts, final_counts, emission_counts = counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:33.060629Z",
     "start_time": "2022-04-05T05:09:33.057336Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:33.739052Z",
     "start_time": "2022-04-05T05:09:33.735940Z"
    }
   },
   "outputs": [],
   "source": [
    "transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:34.098639Z",
     "start_time": "2022-04-05T05:09:34.095480Z"
    }
   },
   "outputs": [],
   "source": [
    "final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:34.482318Z",
     "start_time": "2022-04-05T05:09:34.478856Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emission_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:34.802593Z",
     "start_time": "2022-04-05T05:09:34.799687Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:35.114109Z",
     "start_time": "2022-04-05T05:09:35.111345Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_to_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Checks HMM\n",
    "\n",
    "- Initial counts must sum to the number of sentences  $$ \\sum_{k=1}^K C_{\\text{init}}(c_k) = M$$\n",
    "\n",
    "- Transition counts and Final Counts should sum to the number of tokens: $$\\sum_{k,l=1}^K C_{\\text{trans}}(c_k,c_l)  + \\sum_{k=1}^K C_{\\text{final}}(c_k) = M \\cdot N$$\n",
    "\n",
    "- Emission counts must sum to the number of tokens\n",
    "$$\n",
    "\\sum_{j=1}^J \\sum_{k=1}^K C_{\\text{emiss}}(w_j,c_k) = M \\cdot N \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:37.671334Z",
     "start_time": "2022-04-05T05:09:37.668540Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = len(train_sequences)\n",
    "N = len(train_sequences[0].x)\n",
    "print(\"M:\\t\", M, \"\\nN:\\t\", N,\"\\nM*N:\\t\", M*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:40.259942Z",
     "start_time": "2022-04-05T05:09:40.257021Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"initial_counts sum: \", np.sum(initial_counts))\n",
    "print(\"emission_counts sum: \", np.sum(emission_counts))\n",
    "print(\"transition and final counts sum: \",\\\n",
    "       np.sum(transition_counts) + sum(final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:41.316976Z",
     "start_time": "2022-04-05T05:09:41.313751Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:41.907727Z",
     "start_time": "2022-04-05T05:09:41.904596Z"
    }
   },
   "outputs": [],
   "source": [
    "transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:42.238551Z",
     "start_time": "2022-04-05T05:09:42.235317Z"
    }
   },
   "outputs": [],
   "source": [
    "final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:42.569620Z",
     "start_time": "2022-04-05T05:09:42.566536Z"
    }
   },
   "outputs": [],
   "source": [
    "emission_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From counts to probabilities\n",
    "\n",
    "The following formulas specify how to find the parameters of the HMM:\n",
    "\n",
    "$$\n",
    "P_{\\text{init}}(c_k \\,\\vert\\, \\text{start}) = \\frac{C_{\\text{init}}(c_k)}{ \\sum_{k=1}^K\n",
    "C_{\\text{init}} (c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{final}}(\\text{stop} \\,\\vert\\, c_l) = \\frac{C_{\\text{final}}(c_l) }\n",
    "{\\sum_{k=1}^K C_{\\text{trans}}(c_k,c_l) + C_{\\text{final}}(c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{trans}}( c_k \\,\\vert\\, c_l) = \\frac{C_{\\text{trans}}(c_k, c_l) }\n",
    "{\\sum_{p=1}^K C_{\\text{trans}}(c_p,c_l) + C_{\\text{final}}(c_l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{emiss}} (w_j \\,\\vert\\, c_k) = \\frac{C_{\\text{emiss}} (w_j, c_k) }{\\sum_{q=1}^J C_{\\text{emiss}}(w_q,c_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:43.298823Z",
     "start_time": "2022-04-05T05:09:43.294236Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "transition_probs = transition_counts/(np.sum(transition_counts,0) + final_counts)\n",
    "final_probs      = final_counts/(np.sum(transition_counts, 0) + final_counts )\n",
    "emission_probs   = emission_counts.T / np.sum(emission_counts, 1)\n",
    "\n",
    "print(\"\\ninitial_probs\")\n",
    "print(initial_probs)\n",
    "\n",
    "print(\"\\ntransition_probs\")\n",
    "print(transition_probs)\n",
    "\n",
    "print(\"\\nfinal_probs\")\n",
    "print(final_probs)\n",
    "\n",
    "print(\"\\nemission_probs\")\n",
    "print(emission_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:44.512462Z",
     "start_time": "2022-04-05T05:09:44.509305Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OBSERVATION:\n",
    "\n",
    "**If we stack trainsition and final counts and normalize them we get\n",
    "a proper conditional probability distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:46.941388Z",
     "start_time": "2022-04-05T05:09:46.939289Z"
    }
   },
   "outputs": [],
   "source": [
    "transitions_with_final_counts = np.vstack((transition_counts,\n",
    "                                           final_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:47.260514Z",
     "start_time": "2022-04-05T05:09:47.257201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transitions_with_final_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:47.844831Z",
     "start_time": "2022-04-05T05:09:47.841383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transitions_with_final_counts/ np.sum(transitions_with_final_counts,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with scores \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:49.542383Z",
     "start_time": "2022-04-05T05:09:49.537917Z"
    }
   },
   "outputs": [],
   "source": [
    "def logzero():\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    print(x)\n",
    "    if x == 0:\n",
    "        return logzero()\n",
    "    return np.log(x)\n",
    "\n",
    "\n",
    "def logsum_pair(logx, logy):\n",
    "    \"\"\"\n",
    "    Return log(x+y), avoiding arithmetic underflow/overflow.\n",
    "\n",
    "    logx: log(x)\n",
    "    logy: log(y)\n",
    "\n",
    "    Rationale:\n",
    "\n",
    "    x + y    = e^logx + e^logy\n",
    "             = e^logx (1 + e^(logy-logx))\n",
    "    log(x+y) = logx + log(1 + e^(logy-logx)) (1)\n",
    "\n",
    "    Likewise,\n",
    "    log(x+y) = logy + log(1 + e^(logx-logy)) (2)\n",
    "\n",
    "    The computation of the exponential overflows earlier and is less precise\n",
    "    for big values than for small values. Due to the presence of logy-logx\n",
    "    (resp. logx-logy), (1) is preferred when logx > logy and (2) is preferred\n",
    "    otherwise.\n",
    "    \"\"\"\n",
    "    if logx == logzero():\n",
    "        return logy\n",
    "    elif logx > logy:\n",
    "        return logx + np.log1p(np.exp(logy-logx))\n",
    "    else:\n",
    "        return logy + np.log1p(np.exp(logx-logy))\n",
    "\n",
    "\n",
    "def logsum(logv):\n",
    "    \"\"\"\n",
    "    Return log(v[0]+v[1]+...), avoiding arithmetic underflow/overflow.\n",
    "    \"\"\"\n",
    "    res = logzero()\n",
    "    for val in logv:\n",
    "        res = logsum_pair(res, val)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:49.876913Z",
     "start_time": "2022-04-05T05:09:49.871938Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#a = np.random.rand([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "a = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "\n",
    "print(np.log(sum(np.exp(a))))\n",
    "print(np.log(sum(np.exp(10*a))))\n",
    "print(np.log(sum(np.exp(100*a))))\n",
    "print(np.log(sum(np.exp(1000*a))))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(logsum(a))\n",
    "print(logsum(10*a))\n",
    "print(logsum(100*a))\n",
    "print(logsum(1000*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:50.241729Z",
     "start_time": "2022-04-05T05:09:50.236244Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logsumexp(vec):\n",
    "    c = np.max(vec)\n",
    "    return c + np.log(np.sum(np.exp(vec-c)))\n",
    "\n",
    "#a = np.random.rand([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "a = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10])\n",
    "\n",
    "print(np.log(sum(np.exp(a))))\n",
    "print(np.log(sum(np.exp(10*a))))\n",
    "print(np.log(sum(np.exp(100*a))))\n",
    "print(np.log(sum(np.exp(1000*a))))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(logsumexp(a))\n",
    "print(logsumexp(10*a))\n",
    "print(logsumexp(100*a))\n",
    "print(logsumexp(1000*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:50.861150Z",
     "start_time": "2022-04-05T05:09:50.859158Z"
    }
   },
   "outputs": [],
   "source": [
    "# How can we find this? probabilities will be between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:52.098563Z",
     "start_time": "2022-04-05T05:09:52.052823Z"
    }
   },
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \n",
    "    def __init__(self, word_to_pos={}, state_to_pos={}):\n",
    "        self.fitted = False\n",
    "        self.counts = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.probs  = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.scores = {\"emission\": None, \"transition\":None, \"final\":None, \"initial\":None}\n",
    "        self.decode = set([\"posterior\", \"viterbi\"])\n",
    "        self.word_to_pos  = word_to_pos\n",
    "        self.state_to_pos = state_to_pos\n",
    "        self.pos_to_word  = {v: k for k, v in word_to_pos.items()}\n",
    "        self.pos_to_state = {v: k for k, v in state_to_pos.items()}\n",
    "    \n",
    "        self.n_states     = len(state_to_pos)\n",
    "        self.n_words      = len(word_to_pos)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, observation_lables: list, state_labels: list):\n",
    "        \"\"\"\n",
    "        Computes and saves: counts, probs, scores.\n",
    "        \"\"\"\n",
    "        if self.state_to_pos is None or self.word_to_pos is None:\n",
    "            print(\"Error state_to_pos or word_to_pos needed to be defined\")\n",
    "            return\n",
    "            \n",
    "        self.counts = self.sufficient_statistics_hmm(observation_lables, state_labels)       \n",
    "        self.probs  = self.compute_probs(self.counts)  \n",
    "        self.scores = self.compute_scores(self.probs)  \n",
    "        self.fitted = True\n",
    "        \n",
    "    def sufficient_statistics_hmm(self, observation_lables, state_labels):\n",
    "\n",
    "        state_to_pos, word_to_pos = self.state_to_pos, self.word_to_pos\n",
    "        \n",
    "        def update_initial_counts(initial_counts, seq_x, state_to_pos):\n",
    "            initial_counts[state_to_pos[seq_x[0]]] +=  1\n",
    "            \n",
    "        def update_transition_counts(transition_counts, seq_y, state_to_pos):\n",
    "            for (t_prev, t) in zip(seq_y[:-1], seq_y[1:]):\n",
    "                transition_counts[state_to_pos[t], state_to_pos[t_prev]] += 1 \n",
    "\n",
    "        def update_emission_counts(emission_counts, seq_x, seq_y, state_to_pos, word_to_pos):\n",
    "            for (t,x) in zip(seq_y, seq_x):\n",
    "                emission_counts[state_to_pos[t], word_to_pos[x]] += 1 \n",
    "                \n",
    "        def update_final_counts(final_counts, seq_y, state_to_pos):\n",
    "            final_counts[state_to_pos[seq_y[-1]]] +=1\n",
    "\n",
    "        n_states = len(state_to_pos)\n",
    "        n_words  = len(word_to_pos)\n",
    "        initial_counts      = np.zeros((n_states))\n",
    "        transition_counts   = np.zeros((n_states, n_states))\n",
    "        final_counts        = np.zeros((n_states))\n",
    "        emission_counts     = np.zeros((n_states, n_words))\n",
    "\n",
    "        for seq_x, seq_y in zip(observation_lables, state_labels):\n",
    "            update_initial_counts(initial_counts, seq_y, state_to_pos)\n",
    "            update_transition_counts(transition_counts, seq_y,  state_to_pos)\n",
    "            update_emission_counts(emission_counts, seq_x, seq_y, state_to_pos, word_to_pos) \n",
    "            update_final_counts(final_counts, seq_y,  state_to_pos) \n",
    "\n",
    "        return {\"emission\":   emission_counts, \n",
    "                \"transition\": transition_counts,\n",
    "                \"final\":      final_counts, \n",
    "                \"initial\":    initial_counts}\n",
    "    \n",
    "    def compute_probs(self, counts):\n",
    "        \n",
    "        initial_counts    = counts['initial']\n",
    "        transition_counts = counts['transition']\n",
    "        emission_counts   = counts['emission']\n",
    "        final_counts      = counts['final']\n",
    "\n",
    "        initial_probs    = (initial_counts / np.sum(initial_counts))\n",
    "        transition_probs = transition_counts/(np.sum(transition_counts,0) + final_counts)\n",
    "        final_probs      = final_counts/(np.sum(transition_counts, 0) + final_counts )\n",
    "        emission_probs   = (emission_counts.T / np.sum(emission_counts, 1)).T\n",
    "    \n",
    "        return {\"emission\":   emission_probs, \n",
    "                \"transition\": transition_probs,\n",
    "                \"final\":      final_probs, \n",
    "                \"initial\":    initial_probs}\n",
    "    \n",
    "    def compute_scores(self, probs):\n",
    "         return {\"emission\":   np.log(probs[\"emission\"]), \n",
    "                 \"transition\": np.log(probs[\"transition\"]),\n",
    "                 \"final\":      np.log(probs[\"final\"]), \n",
    "                 \"initial\":    np.log(probs[\"initial\"])}\n",
    "        \n",
    "    def forward_computations(self, x: list):\n",
    "        forward_x = None\n",
    "        return forward_x\n",
    "    \n",
    "    def backward_computations(self, x:list):\n",
    "        backward_x = None\n",
    "        return backward_x\n",
    "    \n",
    "    def log_forward_computations(self, x: list):\n",
    "        \"\"\"\n",
    "        Compute the log_forward computations\n",
    "\n",
    "        Assume there are S possible states and a sequence of length N.\n",
    "        This method will compute iteritavely the log_forward quantities.\n",
    "\n",
    "        * log_f is a S x N Array.\n",
    "        * log_f_x[:,i] will contain the forward quantities at position i.\n",
    "        * log_f_x[:,i] is a vector of size S.\n",
    "        \n",
    "        Returns\n",
    "        - log_f_x: Array of size K x N\n",
    "        \"\"\" \n",
    "        n_x = len(x)\n",
    "        \n",
    "        # log_f_x initialized to -Inf because log(0) = -Inf\n",
    "        log_f_x = np.zeros((self.n_states, n_x)) - np.Inf\n",
    "        x_emission_scores = np.array([hmm.scores['emission'][:, hmm.word_to_pos[w]] for w in x]).T\n",
    "        \n",
    "        log_f_x[:,0] = x_emission_scores[:, 0] + self.scores['initial']\n",
    "        for n in range(1, n_x):\n",
    "            for s in range(self.n_states):\n",
    "                log_f_x[s,n] = logsum(log_f_x[:,n-1] + self.scores['transition'][s,:]) + x_emission_scores[s,n]\n",
    "\n",
    "        log_likelihood = logsum(log_f_x[:,n_x-1] + self.scores['final']) \n",
    "        return log_f_x, log_likelihood\n",
    "    \n",
    "    \n",
    "    def log_backward_computations(self, x: list):\n",
    "        n_x = len(x)\n",
    "        \n",
    "        # log_f_x initialized to -Inf because log(0) = -Inf\n",
    "        log_b_x = np.zeros((self.n_states, n_x)) - np.Inf\n",
    "        x_emission_scores = np.array([hmm.scores['emission'][:, hmm.word_to_pos[w]] for w in x]).T\n",
    "        log_b_x[:,-1] = self.scores['final']\n",
    "\n",
    "        for n in range(n_x-2, -1, -1):\n",
    "            for s in range(self.n_states):\n",
    "                log_b_x[s,n] = logsum(log_b_x[:,n+1] + self.scores['transition'][:,s] + x_emission_scores[:,n+1])\n",
    "\n",
    "        log_likelihood = logsum(log_b_x[:,0] + self.scores['initial'] + x_emission_scores[:,0]) \n",
    "        return log_b_x, log_likelihood\n",
    "        \n",
    "    def predict_labels(self, x: list, decode=\"posterior\"):\n",
    "        \"\"\"\n",
    "        Retuns a sequence of states for each word in **x**.\n",
    "        The output depends on the **decode** method chosen.\n",
    "        \"\"\"\n",
    "        assert decode in self.decode, \"decode `{}` is not valid\".format(decode)\n",
    "        \n",
    "        if decode == 'posterior':\n",
    "            return self.posterior_decode(x)\n",
    "        \n",
    "        if decode == 'viterbi':\n",
    "            return self.viterbi_decode(x)\n",
    "\n",
    "    def compute_state_posteriors(self, x:list):\n",
    "        log_f_x, log_likelihood = self.log_forward_computations(x)\n",
    "        log_b_x, log_likelihood = self.log_backward_computations(x)\n",
    "        state_posteriors = np.zeros((self.n_states, len(x)))\n",
    "        \n",
    "        for pos in range(len(x)):\n",
    "            state_posteriors[:, pos] = log_f_x[:, pos] + log_b_x[:, pos] - log_likelihood\n",
    "        return state_posteriors\n",
    "\n",
    "    def posterior_decode(self, x: list, decode_states=True):\n",
    "        \n",
    "        state_posteriors = self.compute_state_posteriors(x)\n",
    "        y_hat = state_posteriors.argmax(axis=0)\n",
    "        \n",
    "        if decode_states:\n",
    "            y_hat = [hmm.pos_to_state[y] for y in y_hat]\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:52.938066Z",
     "start_time": "2022-04-05T05:09:52.936019Z"
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(word_to_pos, state_to_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:53.346641Z",
     "start_time": "2022-04-05T05:09:53.343403Z"
    }
   },
   "outputs": [],
   "source": [
    "hmm.state_to_pos, hmm.word_to_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:54.019077Z",
     "start_time": "2022-04-05T05:09:54.016848Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [t.x for t in train_sequences]\n",
    "Y = [t.y for t in train_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:54.324615Z",
     "start_time": "2022-04-05T05:09:54.321829Z"
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(word_to_pos, state_to_pos)\n",
    "hmm.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:54.671771Z",
     "start_time": "2022-04-05T05:09:54.668309Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in hmm.counts:\n",
    "    print(k,'\\n', hmm.counts[k],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:55.707205Z",
     "start_time": "2022-04-05T05:09:55.703759Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in hmm.probs:\n",
    "    print(k,'\\n', hmm.probs[k],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:56.250646Z",
     "start_time": "2022-04-05T05:09:56.247076Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in hmm.scores:\n",
    "    print(k,'\\n', hmm.scores[k],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Efficient forward probability computation\n",
    "\n",
    "The forward probability represents the probability that in position\n",
    "$i$ we are in state $Y_i = c_k$ and that we have observed $x_1,\\ldots,x_i$\n",
    "up to that position. Therefore, its mathematical expression is:\n",
    "\\begin{equation}\n",
    "\\mathbf{Forward \\ Probability\\!:}\\;\\;\\;\\;  \\mathrm{forward}(i, c_k) = P(Y_i = c_k, X_1=x_1,\\ldots, X_i = x_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\mathrm{forward}(i, c_k)$ using all the forward computations \\{$\\mathrm{forward}(i -1, c)$ for $c \\in \\Lambda$\\}. In order to facilitate the notation of the following argument we will denote by $x_{i:j}$  the assignemnt $X_i = x_i, \\dots, X_j = x_j$. Therefore we can write   $\\mathrm{forward}(i, y_i) $ as $P( y_i, x_{1:i } ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( y_i, x_{1:i } ) =  \\sum_{y_{i-1} \\in \\Lambda} P( y_i ,y_{i-1}, x_{1:i } )  =  \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) \\cdot P(y_i  | y_{i-1},  x_{1:i-1 }) \\cdot P(y_{i-1},  x_{1:i-1 })  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the **Observation independence** and the **Independence of previous states** properties of the first order HMM we have $P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) = P( x_i  | y_i) $ and $P(y_i  | y_{i-1},  x_{1:i-1 })  = P(y_i  | y_{i-1})  $. Therefore the previous equation can be written, \n",
    "for $i \\in \\{2,\\dots,N\\}$ (where $N$ is the length of the sequence), as \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathrm{forward}(i, y_i)  = \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i, ) \\cdot P(y_i  | y_{i-1}) \\cdot \\mathrm{forward}(i-1, y_{i-1})   \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The previous equation proves that  the forward probability can be defined by the\n",
    "following recurrence rule: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{forward}(1, c_k)&=& P_{\\text{init}}(c_k|\\text{start}) \\times P_{\\mathrm{emiss}}(x_1 | c_k)\n",
    " \\\\\n",
    " \\mathrm{forward}(i, c_k) &=& \\left(  \\sum_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{forward}(i-1, c_l) \\right) \\times P_{\\mathrm{emiss}}(x_i | c_k) \n",
    " \\\\\n",
    "  \\mathrm{forward}(N+1, \\text{stop}) &=& \\sum_{c_l \\in \\Lambda} P_{\\text{final}}(\\text{ stop} | c_l) \\times \\mathrm{forward}(N, c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Using the forward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{forward}(N+1, \\text{ stop}).\n",
    "\\end{equation}\n",
    "\n",
    "Although the forward probability is enough to calculate the likelihood of a given sequence, we will also need the backward probability to calculate the state posteriors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:57.187712Z",
     "start_time": "2022-04-05T05:09:57.184714Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example = train_sequences[1].x\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:57.687386Z",
     "start_time": "2022-04-05T05:09:57.685134Z"
    }
   },
   "outputs": [],
   "source": [
    "log_forward, loglikelihood = hmm.log_forward_computations(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:58.166306Z",
     "start_time": "2022-04-05T05:09:58.162668Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:58.517164Z",
     "start_time": "2022-04-05T05:09:58.513157Z"
    }
   },
   "outputs": [],
   "source": [
    "loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Efficient backward probability computation\n",
    "\n",
    "\n",
    "\n",
    "The backward probability is similar to the forward probability, but operates in the inverse direction.\n",
    "It represents the probability of observing $x_{i+1},\\ldots,x_N$ from position $i+1$ up to $N$, given that at position $i$ we are at state $Y_i = c_l$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Backward \\ Probability\\!:}\\;\\;\\;\\;  \\text{backward}(i, c_l) = P(X_{i+1}=x_{i+1},\\ldots, X_N=x_N | Y_i = c_l).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\text{backward}(i, c_k)$ using all the backward computations $\\text{backward}(i +1, c)$ for $c \\in \\Lambda$.\n",
    "\n",
    "Therefore we can write   $\\text{backward}(i, y_i) $ as $P( x_{i+1:N} | y_i ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( x_{i+1:N} | y_i ) =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+1:N}, y_{i+1} | y_i)  =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+2:N} | y_i, y_{i+1}, x_{i+1}) \n",
    "   P( x_{i+1}, |  y_{i+1},  y_{i}) P( y_{i+1} | y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Using the previous equation we have proved that the backward probability can be defined by the following recurrence rule:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{backward}(N, c_l) &=& P_{\\text{final}}(\\text{stop} | c_l)  \\\\\n",
    "\\text{backward}(i, c_l) &=&  \\displaystyle \\sum_{c_k \\in \\Lambda} P_{\\text{trans}}(c_k | c_l) \\times \n",
    "\\text{backward}(i+1, c_k) \\times P_{\\text{emiss}}(x_{i+1} | c_k) \n",
    " \\\\\n",
    "  \\mathrm{backward}(0, \\text{start}) &=& \\sum_{c_k \\in \\Lambda} P_{\\mathrm{init}}(c_k | \\text{ start}) \\times \\mathrm{backward}(1, c_k) \\times P_{\\mathrm{emiss}}(x_{1} | c_k).\n",
    " \\end{eqnarray}\n",
    "\n",
    "Using the backward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{backward}(0, \\text{start}).\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:59.194332Z",
     "start_time": "2022-04-05T05:09:59.191385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = train_sequences[1].x\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:09:59.823604Z",
     "start_time": "2022-04-05T05:09:59.821160Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_backward, loglikelihood_b = hmm.log_backward_computations(example)\n",
    "log_forward,  loglikelihood_f = hmm.log_forward_computations(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:00.550091Z",
     "start_time": "2022-04-05T05:10:00.546824Z"
    }
   },
   "outputs": [],
   "source": [
    "loglikelihood_b, loglikelihood_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:00.886591Z",
     "start_time": "2022-04-05T05:10:00.883335Z"
    }
   },
   "outputs": [],
   "source": [
    "log_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:01.208107Z",
     "start_time": "2022-04-05T05:10:01.205780Z"
    }
   },
   "outputs": [],
   "source": [
    "state_pos = hmm.compute_state_posteriors(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:01.505760Z",
     "start_time": "2022-04-05T05:10:01.502947Z"
    }
   },
   "outputs": [],
   "source": [
    "state_pos.argmax(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a HMM in the conll data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:02.561930Z",
     "start_time": "2022-04-05T05:10:02.206445Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../skseq/data/CoNLL_2002_shared_task\"\n",
    "train_seq = corpus.read_sequence_list_conll2002(data_path + \"/esp.train\", \n",
    "                                            max_sent_len=100, max_nr_sent=5000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll2002(data_path + \"/esp.testb\",\n",
    "                                           max_sent_len=100, max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll2002(data_path + \"/esp.testa\", \n",
    "                                          max_sent_len=100, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:02.802675Z",
     "start_time": "2022-04-05T05:10:02.797869Z"
    }
   },
   "outputs": [],
   "source": [
    "ind_to_word  = {v: k for k, v in train_seq.x_dict.items()}\n",
    "ind_to_state = {v: k for k, v in train_seq.y_dict.items()}\n",
    "word_to_ind  = train_seq.x_dict\n",
    "state_to_ind = train_seq.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:03.443297Z",
     "start_time": "2022-04-05T05:10:03.422482Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(len(train_seq)):\n",
    "    xy = train_seq[i]\n",
    "    X.append([ind_to_word[x_i] for x_i in xy.x])\n",
    "    Y.append([ind_to_state[y_i] for y_i in xy.y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:04.414661Z",
     "start_time": "2022-04-05T05:10:04.411214Z"
    }
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:07.801048Z",
     "start_time": "2022-04-05T05:10:07.685671Z"
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(word_to_ind, state_to_ind)\n",
    "hmm.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:19:36.613606Z",
     "start_time": "2022-04-05T05:19:36.585510Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = hmm.predict_labels(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:20:04.074908Z",
     "start_time": "2022-04-05T05:20:04.070081Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X[1])\n",
    "print(hmm.predict_labels(X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to make a prediction\n",
    "\n",
    "use `hmm.predict_labels ` to get the set of part of speech tags for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:09.546305Z",
     "start_time": "2022-04-05T05:10:09.519253Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = hmm.predict_labels(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:09.975649Z",
     "start_time": "2022-04-05T05:10:09.972739Z"
    }
   },
   "outputs": [],
   "source": [
    "\" \".join(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:10.426196Z",
     "start_time": "2022-04-05T05:10:10.423169Z"
    }
   },
   "outputs": [],
   "source": [
    "\" \".join(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:10.826998Z",
     "start_time": "2022-04-05T05:10:10.824655Z"
    }
   },
   "outputs": [],
   "source": [
    "result = skseq.sequences.sequence.Sequence(X[0],y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:11.211228Z",
     "start_time": "2022-04-05T05:10:11.208398Z"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure accuracy in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:10:12.120917Z",
     "start_time": "2022-04-05T05:10:12.106945Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.191152Z",
     "start_time": "2022-04-05T05:10:12.652854Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_hat = []\n",
    "for x in tqdm.tqdm(X):\n",
    "    Y_hat.append(hmm.predict_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.323077Z",
     "start_time": "2022-04-05T05:11:11.294147Z"
    }
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "for y,y_hat in zip(Y,Y_hat):\n",
    "    for y_hat_k, y_k in zip(y,y_hat):\n",
    "        total +=1\n",
    "        if y_hat_k == y_k:\n",
    "            correct +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.433949Z",
     "start_time": "2022-04-05T05:11:11.429660Z"
    }
   },
   "outputs": [],
   "source": [
    "correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.550240Z",
     "start_time": "2022-04-05T05:11:11.546721Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy posterior decode train data\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure accuracy in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.679766Z",
     "start_time": "2022-04-05T05:11:11.675809Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:11.805321Z",
     "start_time": "2022-04-05T05:11:11.796253Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(len(test_seq)):\n",
    "    xy = train_seq[i]\n",
    "    X_test.append([ind_to_word[x_i] for x_i in xy.x])\n",
    "    Y_test.append([ind_to_state[y_i] for y_i in xy.y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:23.635391Z",
     "start_time": "2022-04-05T05:11:11.917216Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_test_hat = []\n",
    "for x in tqdm.tqdm(X_test):\n",
    "    Y_test_hat.append(hmm.predict_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:23.644626Z",
     "start_time": "2022-04-05T05:11:23.636776Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_test = 0\n",
    "total_test   = 0\n",
    "for y,y_hat in zip(Y_test,Y_test_hat):\n",
    "    for y_hat_k, y_k in zip(y,y_hat):\n",
    "        total_test +=1\n",
    "        if y_hat_k == y_k:\n",
    "            correct_test +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T05:11:23.757553Z",
     "start_time": "2022-04-05T05:11:23.754362Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy posterior decode test data\", correct_test/total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
